"""
Continuous training module for scheduling and executing model retraining.
"""
import logging
import threading
import time
import queue
import calendar
from datetime import datetime, timedelta
import os
import pandas as pd
import numpy as np

import config
from utils.data_collector import create_data_collector
from utils.data_processor import DataProcessor
from models.model_trainer import ModelTrainer

# Set up logging
logger = logging.getLogger("continuous_trainer")

class ContinuousTrainer:
    """
    Manager for continuous model training.
    Schedules and executes training jobs based on configured schedule.
    """
    def __init__(self):
        """Initialize the continuous training manager."""
        self.data_collector = create_data_collector()
        self.data_processor = DataProcessor()
        self.model_trainer = ModelTrainer()
        
        self.training_thread = None
        self.stop_training = threading.Event()
        self.training_queue = queue.Queue()
        
        self.last_training_time = None
        self.training_in_progress = False
        self.training_history = []
        self.new_data_count = 0
        
        # Log storage for UI display
        self.log_messages = []
        
        # Training progress tracking
        self.current_progress = 0
        self.total_chunks = 0
        self.current_chunk = 0
        
        self.chunk_start_dates = self._generate_monthly_chunks()
        self._add_log("Continuous trainer initialized with schedule: " + config.TRAINING_SCHEDULE['frequency'])
        
    def _generate_monthly_chunks(self):
        """
        Generate a list of date ranges for monthly chunks from the
        configured historical start date to the present.
        
        Returns:
            list: List of (start_date, end_date) tuples for monthly chunks
        """
        if hasattr(config, 'DEFAULT_TRAINING_START_DATE') and config.DEFAULT_TRAINING_START_DATE:
            # S·ª≠ d·ª•ng d·ªØ li·ªáu 12 th√°ng g·∫ßn nh·∫•t cho hu·∫•n luy·ªán
            start = datetime.strptime(config.DEFAULT_TRAINING_START_DATE, "%Y-%m-%d")
            logger.info(f"Using 12-month data for training: starting from {config.DEFAULT_TRAINING_START_DATE}")
            self._add_log(f"üîç S·ª≠ d·ª•ng d·ªØ li·ªáu 12 th√°ng g·∫ßn nh·∫•t cho hu·∫•n luy·ªán (t·ª´ {config.DEFAULT_TRAINING_START_DATE})")
        elif hasattr(config, 'HISTORICAL_START_DATE') and config.HISTORICAL_START_DATE:
            # S·ª≠ d·ª•ng ng√†y b·∫Øt ƒë·∫ßu l·ªãch s·ª≠ c≈© n·∫øu kh√¥ng c√≥ c√†i ƒë·∫∑t m·ªõi
            start = datetime.strptime(config.HISTORICAL_START_DATE, "%Y-%m-%d")
            logger.info(f"Using historical data from {config.HISTORICAL_START_DATE}")
        else:
            return []
            
        end = datetime.now()
        
        chunks = []
        current = start
        
        while current < end:
            # Get the last day of the current month
            _, last_day = calendar.monthrange(current.year, current.month)
            month_end = datetime(current.year, current.month, last_day)
            
            # If this would go beyond the end, use the end date
            if month_end > end:
                month_end = end
                
            # Format for Binance API
            start_date = current.strftime("%Y-%m-%d")
            end_date = month_end.strftime("%Y-%m-%d")
            
            chunks.append((start_date, end_date))
            
            # Move to first day of next month
            current = datetime(current.year, current.month, last_day) + timedelta(days=1)
            
        logger.info(f"Generated {len(chunks)} monthly chunks from {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}")
        return chunks
        
    def start(self):
        """Start the continuous training manager."""
        if self.training_thread is not None and self.training_thread.is_alive():
            logger.warning("Training thread is already running")
            return
            
        # Reset the stop flag
        self.stop_training.clear()
        
        # Create and start the training thread
        self.training_thread = threading.Thread(
            target=self._training_loop, 
            name="ContinuousTrainingThread",
            daemon=True
        )
        self.training_thread.start()
        
        self._add_log("üöÄ Qu√° tr√¨nh hu·∫•n luy·ªán li√™n t·ª•c ƒë√£ ƒë∆∞·ª£c kh·ªüi ƒë·ªông")
        logger.info("Continuous training manager started")
        
    def stop(self):
        """Stop the continuous training manager."""
        if self.training_thread is None or not self.training_thread.is_alive():
            logger.warning("Training thread is not running")
            return
            
        # Set the stop flag
        self.stop_training.set()
        
        # Wait for the thread to finish
        self._add_log("‚è±Ô∏è ƒêang d·ª´ng qu√° tr√¨nh hu·∫•n luy·ªán li√™n t·ª•c...")
        self.training_thread.join(timeout=5.0)
        
        if self.training_thread.is_alive():
            self._add_log("‚ö†Ô∏è Kh√¥ng th·ªÉ d·ª´ng ti·∫øn tr√¨nh hu·∫•n luy·ªán s·∫°ch s·∫Ω")
            logger.warning("Training thread did not stop cleanly")
        else:
            self._add_log("‚úÖ ƒê√£ d·ª´ng qu√° tr√¨nh hu·∫•n luy·ªán li√™n t·ª•c")
            logger.info("Continuous training manager stopped")
            
        self.training_thread = None
        
    def schedule_training(self, force=False):
        """
        Schedule a training job.
        
        Args:
            force (bool): If True, force training regardless of schedule
        """
        if force:
            self._add_log("üîÑ ƒê√£ l√™n l·ªãch hu·∫•n luy·ªán th·ªß c√¥ng")
            logger.info("Forced training scheduled")
            self.training_queue.put("FORCE_TRAIN")
        else:
            self._add_log("üîÑ ƒê√£ l√™n l·ªãch hu·∫•n luy·ªán theo th·ªùi gian ƒë√£ c·∫•u h√¨nh")
            logger.info("Training scheduled according to configured frequency")
            self.training_queue.put("TRAIN")
            
    def _add_log(self, message):
        """Add a log message to the internal log storage for UI display"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"{timestamp} - {message}"
        self.log_messages.append(log_entry)
        # Keep only the most recent 500 messages
        if len(self.log_messages) > 500:
            self.log_messages = self.log_messages[-500:]
            
    def get_training_status(self):
        """
        Get the current training status.
        
        Returns:
            dict: Training status information
        """
        # Calculate progress percentage if training is in progress
        progress = 0
        if self.training_in_progress and self.total_chunks > 0:
            progress = min(100, int((self.current_chunk / self.total_chunks) * 100))
            
        return {
            "enabled": config.CONTINUOUS_TRAINING,
            "in_progress": self.training_in_progress,
            "last_training_time": self.last_training_time.strftime("%Y-%m-%d %H:%M:%S") if self.last_training_time else None,
            "training_history": self.training_history[-10:],  # Last 10 training events
            "new_data_points": self.new_data_count,
            "schedule": config.TRAINING_SCHEDULE,
            "is_training": self.training_in_progress,
            "progress": progress,
            "status": "Training in progress" if self.training_in_progress else "Idle"
        }
        
    def _is_training_scheduled(self):
        """
        Check if training is due according to the configured schedule.
        
        Returns:
            bool: Whether training should occur now
        """
        if not config.CONTINUOUS_TRAINING:
            return False
            
        now = datetime.now()
        schedule = config.TRAINING_SCHEDULE
        
        if schedule["frequency"] == "hourly":
            # Check if the current minute matches the scheduled minute
            return now.minute == schedule["minute"]
            
        elif schedule["frequency"] == "daily":
            # Check if the current hour and minute match the scheduled time
            return (now.hour == schedule["hour"] and 
                    now.minute == schedule["minute"])
                    
        elif schedule["frequency"] == "weekly":
            # Check if the current day, hour, and minute match the scheduled time
            # Note: schedule uses Monday=0, Sunday=6, but datetime uses Monday=0, Sunday=6
            return (now.weekday() == schedule["day_of_week"] and
                    now.hour == schedule["hour"] and 
                    now.minute == schedule["minute"])
        
        return False
        
    def _training_loop(self):
        """Main training loop that runs in a background thread."""
        logger.info("Training loop started")
        
        check_interval = 60  # Check schedule every minute
        
        # Immediately schedule the first training to process historical data
        self.schedule_training(force=True)
        
        # Keep training continuously
        continuous_training_interval = 30 * 60  # 30 minutes between continuous training cycles
        last_continuous_training = time.time()
        
        while not self.stop_training.is_set():
            try:
                # For continuous training, don't wait for scheduled time
                current_time = time.time()
                if current_time - last_continuous_training >= continuous_training_interval:
                    # Start a new training cycle after the interval
                    mins = continuous_training_interval//60
                    self._add_log(f"üïí B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán ƒë·ªãnh k·ª≥ sau {mins} ph√∫t")
                    logger.info(f"Starting continuous training cycle after {mins} minutes")
                    self.schedule_training(force=True)
                    last_continuous_training = current_time
                
                # Check if there's a training job in the queue
                try:
                    # Non-blocking queue check with timeout
                    job = self.training_queue.get(block=True, timeout=1.0)
                    
                    # Process the training job
                    self._execute_training(force=(job == "FORCE_TRAIN"))
                    
                    # Mark job as done
                    self.training_queue.task_done()
                    
                except queue.Empty:
                    # No training job in the queue, continue
                    pass
                    
                # Sleep for a while before checking again
                for _ in range(check_interval):
                    if self.stop_training.is_set():
                        break
                    time.sleep(1)
                    
            except Exception as e:
                logger.error(f"Error in training loop: {e}")
                # Sleep a bit before retrying to avoid tight error loops
                time.sleep(10)
                
        logger.info("Training loop stopped")
        
    def _execute_training(self, force=False):
        """
        Execute the actual training process.
        
        Args:
            force (bool): If True, force training regardless of whether enough new data is available
        """
        if self.training_in_progress:
            logger.warning("Training already in progress, skipping")
            return
            
        try:
            self.training_in_progress = True
            start_time = datetime.now()
            
            logger.info(f"Starting training process at {start_time}")
            
            # Check if we have enough new data to justify retraining
            if not force and self.new_data_count < config.MINIMUM_NEW_DATA_POINTS:
                logger.info(f"Not enough new data ({self.new_data_count} points) for retraining, minimum required: {config.MINIMUM_NEW_DATA_POINTS}")
                self.training_in_progress = False
                return
                
            # Train by monthly chunks if enabled
            if config.CHUNK_BY_MONTHS and self.chunk_start_dates:
                self._train_by_monthly_chunks()
            else:
                self._train_with_all_data()
                
            # Record training event
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            self.last_training_time = end_time
            self.training_history.append({
                "start_time": start_time.strftime("%Y-%m-%d %H:%M:%S"),
                "end_time": end_time.strftime("%Y-%m-%d %H:%M:%S"),
                "duration_seconds": duration,
                "data_points": self.new_data_count,
                "forced": force
            })
            
            # Reset new data counter
            self.new_data_count = 0
            
            # Log completion with duration
            mins = int(duration // 60)
            secs = int(duration % 60)
            time_str = f"{mins} ph√∫t {secs} gi√¢y" if mins > 0 else f"{secs} gi√¢y"
            self._add_log(f"‚úÖ Qu√° tr√¨nh hu·∫•n luy·ªán ho√†n t·∫•t trong {time_str}")
            
            # Calculate next training time
            next_time = end_time + timedelta(seconds=30*60)  # 30 minutes
            self._add_log(f"‚è±Ô∏è ƒê·ª£t hu·∫•n luy·ªán ti·∫øp theo d·ª± ki·∫øn: {next_time.strftime('%H:%M:%S')}")
            
            logger.info(f"Training process completed in {duration:.1f} seconds")
            
        except Exception as e:
            logger.error(f"Error during training execution: {e}")
        finally:
            self.training_in_progress = False
            
    def _train_by_monthly_chunks(self):
        """Train models using monthly data chunks to manage memory usage."""
        logger.info(f"Training with {len(self.chunk_start_dates)} monthly chunks")
        self._add_log(f"B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán v·ªõi {len(self.chunk_start_dates)} ƒëo·∫°n d·ªØ li·ªáu th√°ng")
        
        all_processed_data = []
        # Set total chunks for progress tracking
        self.total_chunks = len(self.chunk_start_dates)
        self.current_chunk = 0
        
        # Process each monthly chunk
        for i, (start_date, end_date) in enumerate(self.chunk_start_dates):
            self.current_chunk = i + 1
            chunk_progress = int((self.current_chunk / self.total_chunks) * 100)
            log_msg = f"ƒêang x·ª≠ l√Ω ƒëo·∫°n d·ªØ li·ªáu {i+1}/{len(self.chunk_start_dates)}: t·ª´ {start_date} ƒë·∫øn {end_date} - {chunk_progress}% ho√†n th√†nh"
            self._add_log(log_msg)
            logger.info(f"Processing chunk {i+1}/{len(self.chunk_start_dates)}: {start_date} to {end_date}")
            
            try:
                # Collect data for this month
                raw_data = self.data_collector.collect_historical_data(
                    timeframe=config.TIMEFRAMES["primary"],
                    start_date=start_date,
                    end_date=end_date
                )
                
                if raw_data is not None and not raw_data.empty:
                    # Process the data
                    processed_chunk = self.data_processor.process_data(raw_data)
                    all_processed_data.append(processed_chunk)
                    self._add_log(f"‚úÖ ƒêo·∫°n {i+1}: ƒê√£ x·ª≠ l√Ω {len(processed_chunk)} ƒëi·ªÉm d·ªØ li·ªáu th√†nh c√¥ng")
                    logger.info(f"Chunk {i+1}: Processed {len(processed_chunk)} data points")
                else:
                    error_msg = f"‚ö†Ô∏è ƒêo·∫°n {i+1}: Kh√¥ng c√≥ d·ªØ li·ªáu cho giai ƒëo·∫°n {start_date} ƒë·∫øn {end_date}"
                    self._add_log(error_msg)
                    logger.warning(f"Chunk {i+1}: No data collected for period {start_date} to {end_date}")
                    
            except Exception as e:
                error_msg = f"‚ùå L·ªói x·ª≠ l√Ω ƒëo·∫°n {i+1}: {str(e)}"
                self._add_log(error_msg)
                logger.error(f"Error processing chunk {i+1}: {e}")
                
        # Combine all processed chunks
        if all_processed_data:
            combined_data = pd.concat(all_processed_data)
            
            # Remove duplicates
            combined_data = combined_data[~combined_data.index.duplicated(keep='last')]
            
            # Sort by time
            combined_data.sort_index(inplace=True)
            
            self._add_log(f"üìä T·ªïng h·ª£p d·ªØ li·ªáu: {len(combined_data)} ƒëi·ªÉm d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω")
            logger.info(f"Combined data: {len(combined_data)} data points")
            
            # Prepare data for different model types
            sequence_data = self.data_processor.prepare_sequence_data(combined_data)
            image_data = self.data_processor.prepare_cnn_data(combined_data)
            
            # Train all models
            self._add_log(f"üß† B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán c√°c m√¥ h√¨nh v·ªõi {len(combined_data)} ƒëi·ªÉm d·ªØ li·ªáu")
            models = self.model_trainer.train_all_models(sequence_data, image_data)
            
            self._add_log(f"‚úÖ ƒê√£ hu·∫•n luy·ªán th√†nh c√¥ng {len(models)} m√¥ h√¨nh")
            logger.info(f"Trained {len(models)} models with chunked data")
        else:
            self._add_log("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu kh·∫£ d·ª•ng sau khi x·ª≠ l√Ω t·∫•t c·∫£ c√°c ƒëo·∫°n")
            logger.error("No processed data available after processing all chunks")
            
    def _train_with_all_data(self):
        """Train models using all data at once."""
        logger.info("Training with all data at once")
        
        try:
            # Collect all historical data
            raw_data = None
            
            self._add_log("üîÑ ƒêang thu th·∫≠p d·ªØ li·ªáu l·ªãch s·ª≠...")
            
            if hasattr(config, 'HISTORICAL_START_DATE') and config.HISTORICAL_START_DATE:
                raw_data = self.data_collector.collect_historical_data(
                    timeframe=config.TIMEFRAMES["primary"],
                    start_date=config.HISTORICAL_START_DATE
                )
            else:
                raw_data = self.data_collector.collect_historical_data(
                    timeframe=config.TIMEFRAMES["primary"],
                    limit=config.LOOKBACK_PERIODS
                )
                
            if raw_data is not None and not raw_data.empty:
                # Process the data
                self._add_log(f"üîß ƒêang x·ª≠ l√Ω {len(raw_data)} ƒëi·ªÉm d·ªØ li·ªáu l·ªãch s·ª≠...")
                processed_data = self.data_processor.process_data(raw_data)
                
                # Prepare data for different model types
                self._add_log("üìä ƒêang chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫ßu v√†o cho c√°c m√¥ h√¨nh...")
                sequence_data = self.data_processor.prepare_sequence_data(processed_data)
                image_data = self.data_processor.prepare_cnn_data(processed_data)
                
                # Train all models
                self._add_log(f"üß† B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán c√°c m√¥ h√¨nh v·ªõi {len(processed_data)} ƒëi·ªÉm d·ªØ li·ªáu")
                models = self.model_trainer.train_all_models(sequence_data, image_data)
                
                self._add_log(f"‚úÖ ƒê√£ hu·∫•n luy·ªán th√†nh c√¥ng {len(models)} m√¥ h√¨nh")
                logger.info(f"Trained {len(models)} models with {len(processed_data)} data points")
            else:
                self._add_log("‚ùå Kh√¥ng th·ªÉ thu th·∫≠p d·ªØ li·ªáu l·ªãch s·ª≠ cho vi·ªác hu·∫•n luy·ªán")
                logger.error("No data collected for training")
                
        except Exception as e:
            self._add_log(f"‚ùå L·ªói hu·∫•n luy·ªán: {str(e)}")
            logger.error(f"Error training with all data: {e}")
            
    def increment_new_data_count(self, count=1):
        """
        Increment the counter of new data points.
        
        Args:
            count (int): Number of new data points to add
        """
        self.new_data_count += count

# Singleton instance
_instance = None

def get_continuous_trainer():
    """
    Get the singleton continuous trainer instance.
    
    Returns:
        ContinuousTrainer: The continuous trainer instance
    """
    global _instance
    if _instance is None:
        _instance = ContinuousTrainer()
    return _instance