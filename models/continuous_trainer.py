"""
Continuous training module for scheduling and executing model retraining.
"""
import logging
import threading
import time
import queue
import calendar
from datetime import datetime, timedelta
import os
import json
import pandas as pd
import numpy as np

import config
from utils.data_collector import create_data_collector
from utils.data_processor import DataProcessor
from models.model_trainer import ModelTrainer

# Set up logging
logger = logging.getLogger("continuous_trainer")

import pandas as pd
import numpy as np

class ContinuousTrainer:
    """
    Manager for continuous model training.
    Schedules and executes training jobs based on configured schedule.
    """
    def __init__(self):
        """Initialize the continuous training manager."""
        self.data_collector = create_data_collector()
        self.data_processor = DataProcessor()
        self.model_trainer = ModelTrainer()
        
        self.training_thread = None
        self.stop_training = threading.Event()
        self.training_queue = queue.Queue()
        
        self.last_training_time = None
        self.training_in_progress = False
        self.training_history = []
        self.new_data_count = 0
        
        # Log storage for UI display
        self.log_messages = []
        
        # Training progress tracking
        self.current_progress = 0
        self.total_chunks = 0
        self.current_chunk = 0
        
        # Danh s√°ch c√°c khung th·ªùi gian c·∫ßn hu·∫•n luy·ªán
        self.timeframes_to_train = [config.PRIMARY_TIMEFRAME, config.SECONDARY_TIMEFRAME]
        
        # Historical start date for training (can be updated at runtime)
        self.historical_start_date = config.HISTORICAL_START_DATE
        # Monthly chunks for training
        self.monthly_chunks = self._generate_monthly_chunks()
        
        # L∆∞u danh s√°ch c√°c ng√†y b·∫Øt ƒë·∫ßu chunks ƒë·ªÉ d√πng sau n√†y
        self.chunk_start_dates = self.monthly_chunks.copy()
        self._add_log("Continuous trainer initialized with schedule: " + config.TRAINING_SCHEDULE['frequency'] + 
                     f" for timeframes: {', '.join(self.timeframes_to_train)}")
        
    def _generate_monthly_chunks(self):
        """
        Generate a list of date ranges for monthly chunks from the
        configured historical start date to the present.
        
        Returns:
            list: List of tuples containing (start_date, end_date) for monthly chunks
        """
        # S·ª≠ d·ª•ng gi√° tr·ªã historical_start_date c·ªßa ƒë·ªëi t∆∞·ª£ng continuous_trainer
        # ho·∫∑c backup t·ª´ config n·∫øu kh√¥ng c√≥
        if hasattr(self, 'historical_start_date') and self.historical_start_date:
            start = datetime.strptime(self.historical_start_date, "%Y-%m-%d")
            logger.info(f"Using custom historical start date: {self.historical_start_date}")
            self._add_log(f"üîç S·ª≠ d·ª•ng ng√†y b·∫Øt ƒë·∫ßu t√πy ch·ªânh: {self.historical_start_date}")
        elif hasattr(config, 'DEFAULT_TRAINING_START_DATE') and config.DEFAULT_TRAINING_START_DATE:
            # S·ª≠ d·ª•ng d·ªØ li·ªáu 12 th√°ng g·∫ßn nh·∫•t cho hu·∫•n luy·ªán
            start = datetime.strptime(config.DEFAULT_TRAINING_START_DATE, "%Y-%m-%d")
            logger.info(f"Using 12-month data for training: starting from {config.DEFAULT_TRAINING_START_DATE}")
            self._add_log(f"üîç S·ª≠ d·ª•ng d·ªØ li·ªáu 12 th√°ng g·∫ßn nh·∫•t cho hu·∫•n luy·ªán (t·ª´ {config.DEFAULT_TRAINING_START_DATE})")
        elif hasattr(config, 'HISTORICAL_START_DATE') and config.HISTORICAL_START_DATE:
            # S·ª≠ d·ª•ng ng√†y b·∫Øt ƒë·∫ßu l·ªãch s·ª≠ c≈© n·∫øu kh√¥ng c√≥ c√†i ƒë·∫∑t m·ªõi
            start = datetime.strptime(config.HISTORICAL_START_DATE, "%Y-%m-%d")
            logger.info(f"Using historical data from {config.HISTORICAL_START_DATE}")
        else:
            return []
            
        end = datetime.now()
        
        chunks = []
        current = start
        
        while current < end:
            # Get the last day of the current month
            _, last_day = calendar.monthrange(current.year, current.month)
            month_end = datetime(current.year, current.month, last_day)
            
            # If this would go beyond the end, use the end date
            if month_end > end:
                month_end = end
                
            # Format for Binance API
            start_date = current.strftime("%Y-%m-%d")
            end_date = month_end.strftime("%Y-%m-%d")
            
            # ƒê·∫£m b·∫£o th√™m tuple c√≥ ƒë√∫ng 2 ph·∫ßn t·ª≠
            chunks.append((start_date, end_date))
            
            # Move to first day of next month
            current = datetime(current.year, current.month, last_day) + timedelta(days=1)
            
        logger.info(f"Generated {len(chunks)} monthly chunks from {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}")
        
        # In ra ki·ªÉm tra ƒë·ªãnh d·∫°ng c√°c chunk
        for i, chunk in enumerate(chunks):
            logger.debug(f"Chunk {i}: {chunk}, type: {type(chunk)}, length: {len(chunk) if isinstance(chunk, tuple) else 'not tuple'}")
            
        return chunks
        
    def start(self):
        """Start the continuous training manager."""
        if self.training_thread is not None and self.training_thread.is_alive():
            logger.warning("Training thread is already running")
            return
            
        # Reset the stop flag
        self.stop_training.clear()
        
        # Create and start the training thread
        self.training_thread = threading.Thread(
            target=self._training_loop, 
            name="ContinuousTrainingThread",
            daemon=True
        )
        self.training_thread.start()
        
        self._add_log("üöÄ Qu√° tr√¨nh hu·∫•n luy·ªán li√™n t·ª•c ƒë√£ ƒë∆∞·ª£c kh·ªüi ƒë·ªông")
        logger.info("Continuous training manager started")
        
    def stop(self):
        """Stop the continuous training manager."""
        if self.training_thread is None or not self.training_thread.is_alive():
            logger.warning("Training thread is not running")
            return
            
        # Set the stop flag
        self.stop_training.set()
        
        # Wait for the thread to finish
        self._add_log("‚è±Ô∏è ƒêang d·ª´ng qu√° tr√¨nh hu·∫•n luy·ªán li√™n t·ª•c...")
        self.training_thread.join(timeout=5.0)
        
        if self.training_thread.is_alive():
            self._add_log("‚ö†Ô∏è Kh√¥ng th·ªÉ d·ª´ng ti·∫øn tr√¨nh hu·∫•n luy·ªán s·∫°ch s·∫Ω")
            logger.warning("Training thread did not stop cleanly")
        else:
            self._add_log("‚úÖ ƒê√£ d·ª´ng qu√° tr√¨nh hu·∫•n luy·ªán li√™n t·ª•c")
            logger.info("Continuous training manager stopped")
            
        self.training_thread = None
        
    def schedule_training(self, force=False):
        """
        Schedule a training job.
        
        Args:
            force (bool): If True, force training regardless of schedule
        """
        if force:
            self._add_log("üîÑ ƒê√£ l√™n l·ªãch hu·∫•n luy·ªán th·ªß c√¥ng")
            logger.info("Forced training scheduled")
            self.training_queue.put("FORCE_TRAIN")
        else:
            self._add_log("üîÑ ƒê√£ l√™n l·ªãch hu·∫•n luy·ªán theo th·ªùi gian ƒë√£ c·∫•u h√¨nh")
            logger.info("Training scheduled according to configured frequency")
            self.training_queue.put("TRAIN")
            
    def _add_log(self, message):
        """Add a log message to the internal log storage for UI display"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"{timestamp} - {message}"
        self.log_messages.append(log_entry)
        # Keep only the most recent 500 messages
        if len(self.log_messages) > 500:
            self.log_messages = self.log_messages[-500:]
            
    def get_training_status(self):
        """
        Get the current training status.
        
        Returns:
            dict: Training status information
        """
        # Calculate progress percentage if training is in progress
        progress = 0
        if self.training_in_progress and self.total_chunks > 0:
            progress = min(100, int((self.current_chunk / self.total_chunks) * 100))
            
        # Th√™m th√¥ng tin v·ªÅ current_chunk v√† total_chunks cho update_status
        status_data = {
            "enabled": config.CONTINUOUS_TRAINING,
            "in_progress": self.training_in_progress,
            "last_training_time": self.last_training_time.strftime("%Y-%m-%d %H:%M:%S") if self.last_training_time else None,
            "training_history": self.training_history[-10:],  # Last 10 training events
            "new_data_points": self.new_data_count,
            "schedule": config.TRAINING_SCHEDULE,
            "is_training": self.training_in_progress,
            "progress": progress,
            "models_trained": self.last_training_time is not None,
            "current_chunk": self.current_chunk,
            "total_chunks": self.total_chunks,
            "status": "Training in progress" if self.training_in_progress else "Idle"
        }
        
        # L∆∞u tr·∫°ng th√°i v√†o file ƒë·ªÉ thread ch√≠nh c√≥ th·ªÉ ƒë·ªçc ƒë∆∞·ª£c
        try:
            import json
            with open("training_status.json", "w") as f:
                json.dump(status_data, f)
        except Exception as e:
            logger.error(f"Error saving training status to file: {e}")
            
        return status_data
        
    def _is_training_scheduled(self):
        """
        Check if training is due according to the configured schedule.
        
        Returns:
            bool: Whether training should occur now
        """
        if not config.CONTINUOUS_TRAINING:
            return False
            
        now = datetime.now()
        schedule = config.TRAINING_SCHEDULE
        
        if schedule["frequency"] == "hourly":
            # Check if the current minute matches the scheduled minute
            return now.minute == schedule["minute"]
            
        elif schedule["frequency"] == "daily":
            # Check if the current hour and minute match the scheduled time
            return (now.hour == schedule["hour"] and 
                    now.minute == schedule["minute"])
                    
        elif schedule["frequency"] == "weekly":
            # Check if the current day, hour, and minute match the scheduled time
            # Note: schedule uses Monday=0, Sunday=6, but datetime uses Monday=0, Sunday=6
            return (now.weekday() == schedule["day_of_week"] and
                    now.hour == schedule["hour"] and 
                    now.minute == schedule["minute"])
        
        return False
        
    def _training_loop(self):
        """Main training loop that runs in a background thread."""
        logger.info("Training loop started")
        
        check_interval = 60  # Check schedule every minute
        
        # Immediately schedule the first training to process historical data
        self.schedule_training(force=True)
        
        # Keep training continuously
        continuous_training_interval = 30 * 60  # 30 minutes between continuous training cycles
        last_continuous_training = time.time()
        
        while not self.stop_training.is_set():
            try:
                # For continuous training, don't wait for scheduled time
                current_time = time.time()
                if current_time - last_continuous_training >= continuous_training_interval:
                    # Start a new training cycle after the interval
                    mins = continuous_training_interval//60
                    self._add_log(f"üïí B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán ƒë·ªãnh k·ª≥ sau {mins} ph√∫t")
                    logger.info(f"Starting continuous training cycle after {mins} minutes")
                    self.schedule_training(force=True)
                    last_continuous_training = current_time
                
                # Check if there's a training job in the queue
                try:
                    # Non-blocking queue check with timeout
                    job = self.training_queue.get(block=True, timeout=1.0)
                    
                    # Process the training job
                    self._execute_training(force=(job == "FORCE_TRAIN"))
                    
                    # Mark job as done
                    self.training_queue.task_done()
                    
                except queue.Empty:
                    # No training job in the queue, continue
                    pass
                    
                # Sleep for a while before checking again
                for _ in range(check_interval):
                    if self.stop_training.is_set():
                        break
                    time.sleep(1)
                    
            except Exception as e:
                logger.error(f"Error in training loop: {e}")
                # Sleep a bit before retrying to avoid tight error loops
                time.sleep(10)
                
        logger.info("Training loop stopped")
        
    def _execute_training(self, force=False):
        """
        Execute the actual training process.
        
        Args:
            force (bool): If True, force training regardless of whether enough new data is available
        """
        if self.training_in_progress:
            logger.warning("Training already in progress, skipping")
            return
            
        try:
            self.training_in_progress = True
            start_time = datetime.now()
            
            logger.info(f"Starting training process at {start_time}")
            
            # Check if we have enough new data to justify retraining
            if not force and self.new_data_count < config.MINIMUM_NEW_DATA_POINTS:
                logger.info(f"Not enough new data ({self.new_data_count} points) for retraining, minimum required: {config.MINIMUM_NEW_DATA_POINTS}")
                self.training_in_progress = False
                return
            
            # L∆∞u tr·ªØ k·∫øt qu·∫£ m√¥ h√¨nh    
            model_results = {}
                
            # Train by monthly chunks if enabled
            try:
                if config.CHUNK_BY_MONTHS and self.chunk_start_dates:
                    # S·ª¨A L·ªñI: B·ªçc trong try-except ƒë·ªÉ x·ª≠ l√Ω l·ªói "too many values to unpack"
                    model_results = self._train_by_monthly_chunks()
                else:
                    # C√≥ th·ªÉ _train_with_all_data c≈©ng tr·∫£ v·ªÅ model_results
                    model_results = self._train_with_all_data()
            except ValueError as e:
                if "too many values to unpack" in str(e):
                    logger.error(f"Error in _execute_training: {e}")
                    self._add_log(f"‚ö†Ô∏è L·ªói khi th·ª±c thi hu·∫•n luy·ªán: {e}")
                    # T·∫°o model_results r·ªóng ƒë·ªÉ tr√°nh l·ªói khi truy c·∫≠p
                    model_results = {tf: {} for tf in self.timeframes_to_train}
                else:
                    # N√©m l·∫°i ngo·∫°i l·ªá n·∫øu kh√¥ng ph·∫£i l·ªói unpacking
                    raise
                
            # Ghi nh·∫≠t k√Ω k·∫øt qu·∫£
            if model_results:
                timeframes = list(model_results.keys())
                models_count = sum(len(models) for models in model_results.values() if isinstance(models, dict))
                self._add_log(f"‚úÖ ƒê√£ hu·∫•n luy·ªán th√†nh c√¥ng {models_count} m√¥ h√¨nh cho {len(timeframes)} khung th·ªùi gian")
                logger.info(f"Trained {models_count} models for {timeframes}")
                
            # Record training event
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            self.last_training_time = end_time
            self.training_history.append({
                "start_time": start_time.strftime("%Y-%m-%d %H:%M:%S"),
                "end_time": end_time.strftime("%Y-%m-%d %H:%M:%S"),
                "duration_seconds": duration,
                "data_points": self.new_data_count,
                "forced": force
            })
            
            # Reset new data counter
            self.new_data_count = 0
            
            # Log completion with duration
            mins = int(duration // 60)
            secs = int(duration % 60)
            time_str = f"{mins} ph√∫t {secs} gi√¢y" if mins > 0 else f"{secs} gi√¢y"
            self._add_log(f"‚úÖ Qu√° tr√¨nh hu·∫•n luy·ªán ho√†n t·∫•t trong {time_str}")
            
            # Calculate next training time
            next_time = end_time + timedelta(seconds=30*60)  # 30 minutes
            self._add_log(f"‚è±Ô∏è ƒê·ª£t hu·∫•n luy·ªán ti·∫øp theo d·ª± ki·∫øn: {next_time.strftime('%H:%M:%S')}")
            
            logger.info(f"Training process completed in {duration:.1f} seconds")
            
        except Exception as e:
            logger.error(f"Error during training execution: {e}")
        finally:
            self.training_in_progress = False
            
    def _train_by_monthly_chunks(self):
        """
        Train models using monthly data chunks to manage memory usage for both timeframes.
        
        Returns:
            dict: Dictionary c·ªßa c√°c m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán cho m·ªói khung th·ªùi gian
        """
        logger.info(f"Training with {len(self.monthly_chunks)} monthly chunks from {self.historical_start_date} for timeframes: {', '.join(self.timeframes_to_train)}")
        self._add_log(f"B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán v·ªõi {len(self.monthly_chunks)} ƒëo·∫°n d·ªØ li·ªáu th√°ng t·ª´ {self.historical_start_date}")
        
        # Dictionary ƒë·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω cho m·ªói khung th·ªùi gian
        all_processed_data = {timeframe: [] for timeframe in self.timeframes_to_train}
        
        # Dictionary ƒë·ªÉ l∆∞u tr·ªØ k·∫øt qu·∫£ m√¥ h√¨nh cho m·ªói khung th·ªùi gian
        model_results = {}
        
        # Set total chunks for progress tracking (t·ªïng s·ªë chunks nh√¢n v·ªõi s·ªë khung th·ªùi gian)
        self.total_chunks = len(self.monthly_chunks) * len(self.timeframes_to_train)
        self.current_chunk = 0
        
        # Ki·ªÉm tra xem ƒë√£ c√≥ d·ªØ li·ªáu ƒë√£ t·∫£i tr∆∞·ªõc ƒë√≥ ch∆∞a
        existing_data_ranges = self._get_existing_data_ranges()
        
        # X·ª≠ l√Ω t·ª´ng khung th·ªùi gian
        for timeframe in self.timeframes_to_train:
            self._add_log(f"üïí ƒêang x·ª≠ l√Ω d·ªØ li·ªáu cho khung th·ªùi gian: {timeframe}")
            logger.info(f"Processing data for timeframe: {timeframe}")
            
            # Process each monthly chunk for this timeframe
            for i, chunk in enumerate(self.monthly_chunks):
                # ƒê·∫£m b·∫£o chunk l√† tuple v·ªõi 2 ph·∫ßn t·ª≠
                if isinstance(chunk, tuple) and len(chunk) == 2:
                    start_date, end_date = chunk
                else:
                    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p chunk kh√¥ng ph·∫£i l√† tuple 2 ph·∫ßn t·ª≠
                    self._add_log(f"‚ö†Ô∏è ƒê·ªãnh d·∫°ng chunk kh√¥ng h·ª£p l·ªá: {chunk}")
                    logger.warning(f"Invalid chunk format: {chunk}")
                    continue

                self.current_chunk += 1
                chunk_progress = int((self.current_chunk / self.total_chunks) * 100)
                
                # Kh√≥a t√†i nguy√™n cho khung th·ªùi gian v√† kho·∫£ng th·ªùi gian c·ª• th·ªÉ
                cache_key = f"{timeframe}_{start_date}_{end_date}"
                
                # Ki·ªÉm tra xem d·ªØ li·ªáu cho kho·∫£ng th·ªùi gian n√†y ƒë√£ ƒë∆∞·ª£c t·∫£i tr∆∞·ªõc ƒë√≥ ch∆∞a
                if self._is_data_range_covered(start_date, end_date, existing_data_ranges):
                    # D·ªØ li·ªáu ƒë√£ t·ªìn t·∫°i, s·ª≠ d·ª•ng l·∫°i
                    log_msg = f"‚è© B·ªè qua ƒëo·∫°n {i+1}/{len(self.monthly_chunks)} ({timeframe}): t·ª´ {start_date} ƒë·∫øn {end_date} - ƒë√£ c√≥ d·ªØ li·ªáu"
                    self._add_log(log_msg)
                    logger.info(f"Skipping chunk {i+1}/{len(self.monthly_chunks)} ({timeframe}): {start_date} to {end_date} - data already exists")
                    
                    # T·∫£i d·ªØ li·ªáu ƒë√£ l∆∞u t·ª´ t·ªáp cache
                    try:
                        cached_data = self._load_cached_data(start_date, end_date, timeframe)
                        if cached_data is not None and not cached_data.empty:
                            if timeframe not in all_processed_data:
                                all_processed_data[timeframe] = []
                            all_processed_data[timeframe].append(cached_data)
                            self._add_log(f"‚úÖ ƒêo·∫°n {i+1} ({timeframe}): ƒê√£ t·∫£i {len(cached_data)} ƒëi·ªÉm d·ªØ li·ªáu t·ª´ b·ªô nh·ªõ ƒë·ªám")
                    except Exception as e:
                        # N·∫øu kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu t·ª´ cache, t·∫£i l·∫°i t·ª´ API
                        log_msg = f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu ƒë·ªám cho ƒëo·∫°n {i+1} ({timeframe}): {str(e)} - ƒêang t·∫£i l·∫°i t·ª´ Binance"
                        self._add_log(log_msg)
                        logger.warning(f"Could not load cached data for chunk {i+1} ({timeframe}): {e} - Redownloading")
                        # Ti·∫øp t·ª•c v·ªõi quy tr√¨nh t·∫£i m·ªõi d∆∞·ªõi ƒë√¢y
                
                # N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªám ho·∫∑c kh√¥ng th·ªÉ t·∫£i, t·∫£i m·ªõi t·ª´ API
                if len(all_processed_data[timeframe]) <= i:
                    log_msg = f"üì• ƒêang t·∫£i ƒëo·∫°n d·ªØ li·ªáu {i+1}/{len(self.monthly_chunks)} ({timeframe}): t·ª´ {start_date} ƒë·∫øn {end_date} - {chunk_progress}% ho√†n th√†nh"
                    self._add_log(log_msg)
                    logger.info(f"Downloading chunk {i+1}/{len(self.monthly_chunks)} ({timeframe}): {start_date} to {end_date}")
                
                    try:
                        # Collect data for this month with the specific timeframe
                        raw_data = self.data_collector.collect_historical_data(
                            timeframe=timeframe,
                            start_date=start_date,
                            end_date=end_date
                        )
                        
                        if raw_data is not None and not raw_data.empty:
                            # Process the data
                            processed_chunk = self.data_processor.process_data(raw_data)
                            all_processed_data[timeframe].append(processed_chunk)
                            
                            # L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o b·ªô nh·ªõ ƒë·ªám v·ªõi khung th·ªùi gian
                            self._save_cached_data(processed_chunk, start_date, end_date, timeframe=timeframe)
                            
                            # C·∫≠p nh·∫≠t danh s√°ch c√°c kho·∫£ng th·ªùi gian ƒë√£ t·∫£i
                            existing_data_ranges.append((start_date, end_date, timeframe))
                            
                            self._add_log(f"‚úÖ ƒêo·∫°n {i+1} ({timeframe}): ƒê√£ x·ª≠ l√Ω {len(processed_chunk)} ƒëi·ªÉm d·ªØ li·ªáu th√†nh c√¥ng")
                            logger.info(f"Chunk {i+1} ({timeframe}): Processed {len(processed_chunk)} data points")
                        else:
                            error_msg = f"‚ö†Ô∏è ƒêo·∫°n {i+1} ({timeframe}): Kh√¥ng c√≥ d·ªØ li·ªáu cho giai ƒëo·∫°n {start_date} ƒë·∫øn {end_date}"
                            self._add_log(error_msg)
                            logger.warning(f"Chunk {i+1} ({timeframe}): No data collected for period {start_date} to {end_date}")
                            
                    except Exception as e:
                        error_msg = f"‚ùå L·ªói x·ª≠ l√Ω ƒëo·∫°n {i+1} ({timeframe}): {str(e)}"
                        self._add_log(error_msg)
                        logger.error(f"Error processing chunk {i+1} ({timeframe}): {e}")
        
        # Sau khi x·ª≠ l√Ω to√†n b·ªô d·ªØ li·ªáu cho t·∫•t c·∫£ c√°c khung th·ªùi gian
        for timeframe, data_chunks in all_processed_data.items():
            if data_chunks:
                # K·∫øt h·ª£p t·∫•t c·∫£ c√°c ƒëo·∫°n d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω cho khung th·ªùi gian n√†y
                combined_data = pd.concat(data_chunks)
                
                # Lo·∫°i b·ªè c√°c d√≤ng tr√πng l·∫∑p
                combined_data = combined_data[~combined_data.index.duplicated(keep='last')]
                
                # S·∫Øp x·∫øp theo th·ªùi gian
                combined_data.sort_index(inplace=True)
                
                self._add_log(f"üìä T·ªïng h·ª£p d·ªØ li·ªáu ({timeframe}): {len(combined_data)} ƒëi·ªÉm d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω")
                logger.info(f"Combined data for {timeframe}: {len(combined_data)} data points")
                
                # Chu·∫©n b·ªã d·ªØ li·ªáu cho c√°c lo·∫°i m√¥ h√¨nh kh√°c nhau
                sequence_data = self.data_processor.prepare_sequence_data(combined_data)
                image_data = self.data_processor.prepare_cnn_data(combined_data)
                
                # Hu·∫•n luy·ªán t·∫•t c·∫£ c√°c m√¥ h√¨nh v·ªõi khung th·ªùi gian c·ª• th·ªÉ
                self._add_log(f"üß† B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán c√°c m√¥ h√¨nh cho {timeframe} v·ªõi {len(combined_data)} ƒëi·ªÉm d·ªØ li·ªáu")
                
                # L∆∞u th√¥ng tin v·ªÅ khung th·ªùi gian v√†o d·ªØ li·ªáu hu·∫•n luy·ªán
                for data_dict in [sequence_data, image_data]:
                    for key in data_dict:
                        if isinstance(data_dict[key], dict):
                            data_dict[key]['timeframe'] = timeframe
                
                # S·ª¨A L·ªñI: X·ª≠ l√Ω l·ªói "too many values to unpack (expected 2)"
                try:
                    # Fix #1: L∆∞u k·∫øt qu·∫£ v√†o bi·∫øn t·∫°m tr∆∞·ªõc ƒë·ªÉ ki·ªÉm tra lo·∫°i d·ªØ li·ªáu
                    result = self.model_trainer.train_all_models(sequence_data, image_data, timeframe=timeframe)
                    
                    # Fix #2: Ki·ªÉm tra xem k·∫øt qu·∫£ c√≥ ph·∫£i l√† tuple kh√¥ng, n·∫øu c√≥ th√¨ l·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n
                    if isinstance(result, tuple) and len(result) > 0:
                        models = result[0]  # L·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n (models)
                        self._add_log(f"‚ö†Ô∏è ƒê√£ t·ª± ƒë·ªông x·ª≠ l√Ω k·∫øt qu·∫£ tuple t·ª´ train_all_models")
                    else:
                        # N·∫øu kh√¥ng ph·∫£i tuple, s·ª≠ d·ª•ng k·∫øt qu·∫£ tr·ª±c ti·∫øp
                        models = result
                        
                    # Fix #3: ƒê·∫£m b·∫£o models kh√¥ng None tr∆∞·ªõc khi l∆∞u v√†o k·∫øt qu·∫£
                    if models is not None:
                        model_results[timeframe] = models
                        self._add_log(f"‚úÖ ƒê√£ hu·∫•n luy·ªán th√†nh c√¥ng m√¥ h√¨nh cho {timeframe}")
                    else:
                        self._add_log(f"‚ö†Ô∏è Hu·∫•n luy·ªán cho {timeframe} tr·∫£ v·ªÅ None")
                        model_results[timeframe] = {}
                        
                except ValueError as e:
                    # Fix #4: X·ª≠ l√Ω l·ªói 'too many values to unpack' n·∫øu v·∫´n x·∫£y ra
                    if "too many values to unpack" in str(e):
                        self._add_log(f"‚ö†Ô∏è L·ªói ƒë·ªãnh d·∫°ng k·∫øt qu·∫£: {str(e)}")
                        logger.warning(f"Value unpacking error in train_all_models: {str(e)}")
                        
                        try:
                            # L·∫•y k·∫øt qu·∫£ tr·ª±c ti·∫øp, kh√¥ng chuy·ªÉn sang list
                            result = self.model_trainer.train_all_models(sequence_data, image_data, timeframe=timeframe)
                            
                            # Ki·ªÉm tra tr∆∞·ªùng h·ª£p k·∫øt qu·∫£ l√† m·ªôt tuple
                            if isinstance(result, tuple) and len(result) > 0:
                                models = result[0]  # L·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n n·∫øu l√† tuple
                            else:
                                models = result  # S·ª≠ d·ª•ng k·∫øt qu·∫£ tr·ª±c ti·∫øp n·∫øu kh√¥ng ph·∫£i tuple
                                
                            if models is not None:
                                model_results[timeframe] = models
                                self._add_log(f"‚úÖ ƒê√£ kh·∫Øc ph·ª•c l·ªói v√† hu·∫•n luy·ªán th√†nh c√¥ng m√¥ h√¨nh cho {timeframe}")
                            else:
                                self._add_log(f"‚ö†Ô∏è K·∫øt qu·∫£ hu·∫•n luy·ªán r·ªóng cho {timeframe}")
                                model_results[timeframe] = {}
                        except Exception as inner_e:
                            self._add_log(f"‚ùå L·ªói khi x·ª≠ l√Ω k·∫øt qu·∫£ hu·∫•n luy·ªán: {str(inner_e)}")
                            logger.error(f"Error processing training result: {inner_e}")
                            model_results[timeframe] = {}
                    else:
                        # L·ªói ValueError kh√°c
                        self._add_log(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)}")
                        logger.error(f"Unknown error in train_all_models: {e}")
                        model_results[timeframe] = {}
                except Exception as e:
                    # Fix #5: X·ª≠ l√Ω c√°c ngo·∫°i l·ªá kh√°c
                    self._add_log(f"‚ùå L·ªói trong qu√° tr√¨nh hu·∫•n luy·ªán: {str(e)}")
                    logger.error(f"Error in training process: {e}")
                    model_results[timeframe] = {}
                
                # Th√¥ng b√°o k·∫øt qu·∫£ sau khi x·ª≠ l√Ω
                if timeframe in model_results and model_results[timeframe]:
                    try:
                        model_count = len(model_results[timeframe])
                        self._add_log(f"‚úÖ ƒê√£ hu·∫•n luy·ªán th√†nh c√¥ng {model_count} m√¥ h√¨nh cho {timeframe}")
                    except:
                        self._add_log(f"‚úÖ ƒê√£ hu·∫•n luy·ªán th√†nh c√¥ng m√¥ h√¨nh cho {timeframe}")
                else:
                    self._add_log(f"‚ö†Ô∏è Kh√¥ng c√≥ m√¥ h√¨nh n√†o ƒë∆∞·ª£c hu·∫•n luy·ªán cho {timeframe}")
                logger.info(f"Trained models for {timeframe} with {len(combined_data)} data points")
            else:
                self._add_log(f"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu kh·∫£ d·ª•ng cho {timeframe} sau khi x·ª≠ l√Ω t·∫•t c·∫£ c√°c ƒëo·∫°n")
                logger.error(f"No processed data available for {timeframe} after processing all chunks")
        
        # Tr·∫£ v·ªÅ k·∫øt qu·∫£ hu·∫•n luy·ªán cho t·∫•t c·∫£ c√°c khung th·ªùi gian
        return model_results
    
    def _get_existing_data_ranges(self):
        """
        Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ t·∫£i t·ª´ tr∆∞·ªõc ƒë√≥
        
        Returns:
            list: Danh s√°ch c√°c kho·∫£ng th·ªùi gian ƒë√£ t·∫£i, d·∫°ng [(start_date, end_date), ...]
        """
        # Ki·ªÉm tra th∆∞ m·ª•c l∆∞u tr·ªØ d·ªØ li·ªáu
        cache_dir = os.path.join(config.MODEL_DIR, "data_cache")
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir, exist_ok=True)
            return []
        
        # ƒê·ªçc danh s√°ch kho·∫£ng th·ªùi gian ƒë√£ t·∫£i
        ranges_file = os.path.join(cache_dir, "data_ranges.json")
        if os.path.exists(ranges_file):
            try:
                with open(ranges_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Error loading existing data ranges: {e}")
                return []
        return []
    
    def _is_data_range_covered(self, start_date, end_date, existing_ranges):
        """
        Ki·ªÉm tra xem kho·∫£ng th·ªùi gian ƒë√£ ƒë∆∞·ª£c t·∫£i tr∆∞·ªõc ƒë√≥ ch∆∞a
        
        Args:
            start_date (str): Ng√†y b·∫Øt ƒë·∫ßu kho·∫£ng th·ªùi gian m·ªõi
            end_date (str): Ng√†y k·∫øt th√∫c kho·∫£ng th·ªùi gian m·ªõi
            existing_ranges (list): Danh s√°ch c√°c kho·∫£ng th·ªùi gian ƒë√£ t·∫£i
            
        Returns:
            bool: True n·∫øu kho·∫£ng th·ªùi gian ƒë√£ ƒë∆∞·ª£c t·∫£i, False n·∫øu ch∆∞a
        """
        if not existing_ranges:
            return False
            
        try:
            start = datetime.strptime(start_date, "%Y-%m-%d")
            end = datetime.strptime(end_date, "%Y-%m-%d")
            
            for data_range in existing_ranges:
                try:
                    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p data_range l√† dict (ƒë·ªãnh d·∫°ng m·ªõi)
                    if isinstance(data_range, dict) and 'start_date' in data_range and 'end_date' in data_range:
                        exist_start = data_range['start_date']
                        exist_end = data_range['end_date']
                        
                        # Ki·ªÉm tra ƒë·ªãnh d·∫°ng kh·ªõp v·ªõi timeframe
                        if hasattr(self, 'current_timeframe') and hasattr(data_range, 'timeframe'):
                            if self.current_timeframe != data_range.get('timeframe'):
                                continue
                                
                        exist_start_date = datetime.strptime(exist_start, "%Y-%m-%d")
                        exist_end_date = datetime.strptime(exist_end, "%Y-%m-%d")
                        
                        # Kho·∫£ng th·ªùi gian ƒë√£ ƒë∆∞·ª£c bao ph·ªß b·ªüi m·ªôt kho·∫£ng th·ªùi gian hi·ªán c√≥
                        if start >= exist_start_date and end <= exist_end_date:
                            logger.info(f"Kho·∫£ng th·ªùi gian {start_date} ƒë·∫øn {end_date} ƒë√£ t·ªìn t·∫°i trong cache")
                            return True
                    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p data_range l√† list/tuple (ƒë·ªãnh d·∫°ng c≈©)
                    elif isinstance(data_range, (list, tuple)) and len(data_range) >= 2:
                        exist_start = data_range[0]
                        exist_end = data_range[1]
                        
                        # Ki·ªÉm tra ƒë·ªãnh d·∫°ng kh·ªõp v·ªõi timeframe
                        if len(data_range) >= 3:
                            range_timeframe = data_range[2]
                            # B·ªè qua n·∫øu kh√¥ng c√πng timeframe
                            if hasattr(self, 'current_timeframe') and self.current_timeframe != range_timeframe:
                                continue
                                
                        exist_start_date = datetime.strptime(exist_start, "%Y-%m-%d")
                        exist_end_date = datetime.strptime(exist_end, "%Y-%m-%d")
                        
                        # Kho·∫£ng th·ªùi gian ƒë√£ ƒë∆∞·ª£c bao ph·ªß b·ªüi m·ªôt kho·∫£ng th·ªùi gian hi·ªán c√≥
                        if start >= exist_start_date and end <= exist_end_date:
                            logger.info(f"Kho·∫£ng th·ªùi gian {start_date} ƒë·∫øn {end_date} ƒë√£ t·ªìn t·∫°i trong cache")
                            return True
                except (ValueError, IndexError, TypeError, KeyError) as e:
                    logger.warning(f"L·ªói khi x·ª≠ l√Ω kho·∫£ng d·ªØ li·ªáu {data_range}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"L·ªói khi ki·ªÉm tra kho·∫£ng d·ªØ li·ªáu {start_date} - {end_date}: {e}")
            # Tr∆∞·ªõc ƒë√¢y tr·∫£ v·ªÅ gi√° tr·ªã 0 g√¢y l·ªói "too many values to unpack (expected 2)"
            # Tr·∫£ v·ªÅ False ƒë·ªÉ x·ª≠ l√Ω l·ªói m·ªôt c√°ch an to√†n
            return False
            
        # Tr·∫£ v·ªÅ False n·∫øu kh√¥ng t√¨m th·∫•y kho·∫£ng th·ªùi gian ph√π h·ª£p
        return False
    
    def _save_cached_data(self, data, start_date, end_date, timeframe=None):
        """
        L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o b·ªô nh·ªõ ƒë·ªám v·ªõi t√≠nh nƒÉng n√©n ƒë·ªÉ ti·∫øt ki·ªám kh√¥ng gian.
        
        Args:
            data (pd.DataFrame): D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
            start_date (str): Ng√†y b·∫Øt ƒë·∫ßu kho·∫£ng th·ªùi gian
            end_date (str): Ng√†y k·∫øt th√∫c kho·∫£ng th·ªùi gian
            timeframe (str, optional): Khung th·ªùi gian c·ªßa d·ªØ li·ªáu
        """
        try:
            # ƒê·∫£m b·∫£o th∆∞ m·ª•c cache t·ªìn t·∫°i
            cache_dir = os.path.join(config.MODEL_DIR, "data_cache")
            os.makedirs(cache_dir, exist_ok=True)
            
            # T·∫°o t√™n t·ªáp d·ª±a tr√™n kho·∫£ng th·ªùi gian v√† khung th·ªùi gian
            file_name = f"{start_date}_to_{end_date}"
            if timeframe:
                file_name += f"_{timeframe}"
            cache_file = os.path.join(cache_dir, f"{file_name}.pkl.gz")
            
            # T·ªëi ∆∞u h√≥a ki·ªÉu d·ªØ li·ªáu tr∆∞·ªõc khi l∆∞u ƒë·ªÉ gi·∫£m k√≠ch th∆∞·ªõc
            optimized_data = self._optimize_dataframe_types(data.copy())
            
            # L∆∞u DataFrame v√†o t·ªáp v·ªõi t√≠nh nƒÉng n√©n
            optimized_data.to_pickle(cache_file, compression='gzip')
            
            # T√≠nh k√≠ch th∆∞·ªõc ƒë√£ ti·∫øt ki·ªám
            normal_size = data.memory_usage(deep=True).sum()
            optimized_size = optimized_data.memory_usage(deep=True).sum()
            savings_percent = ((normal_size - optimized_size) / normal_size * 100) if normal_size > 0 else 0
            
            # C·∫≠p nh·∫≠t danh s√°ch kho·∫£ng th·ªùi gian ƒë√£ t·∫£i
            ranges_file = os.path.join(cache_dir, "data_ranges.json")
            existing_ranges = self._get_existing_data_ranges()
            
            # Th√™m kho·∫£ng th·ªùi gian m·ªõi k√®m th√¥ng tin v·ªÅ k√≠ch th∆∞·ªõc v√† ng√†y t·∫°o
            if not self._is_data_range_covered(start_date, end_date, existing_ranges):
                existing_ranges.append({
                    "start_date": start_date,
                    "end_date": end_date,
                    "created_at": datetime.now().isoformat(),
                    "size_bytes": os.path.getsize(cache_file) if os.path.exists(cache_file) else 0,
                    "rows": len(optimized_data)
                })
                
                # L∆∞u danh s√°ch c·∫≠p nh·∫≠t
                with open(ranges_file, 'w') as f:
                    json.dump(existing_ranges, f, indent=2)
            
            logger.info(f"Cached data saved for period {start_date} to {end_date} (Memory optimized: {savings_percent:.1f}% saved)")
        except Exception as e:
            logger.error(f"Error saving cached data: {e}")
    
    def _optimize_dataframe_types(self, df):
        """
        T·ªëi ∆∞u ki·ªÉu d·ªØ li·ªáu c·ªßa DataFrame ƒë·ªÉ gi·∫£m b·ªô nh·ªõ s·ª≠ d·ª•ng.
        
        Args:
            df (pd.DataFrame): DataFrame c·∫ßn t·ªëi ∆∞u
            
        Returns:
            pd.DataFrame: DataFrame ƒë√£ t·ªëi ∆∞u
        """
        # Danh s√°ch c√°c c·ªôt ƒë·ªÉ b·ªè qua qu√° tr√¨nh t·ªëi ∆∞u (c√°c c·ªôt m·ª•c ti√™u)
        exclude_cols = ['target', 'target_class', 'target_binary']
        
        try:
            # Lo·∫°i b·ªè c√°c gi√° tr·ªã NaN tr∆∞·ªõc khi t·ªëi ∆∞u ki·ªÉu d·ªØ li·ªáu
            df = df.fillna(0)
            
            # Lo·∫°i b·ªè c√°c gi√° tr·ªã v√¥ c√πng (inf)
            df = df.replace([np.inf, -np.inf], 0)
            
            # T·ªëi ∆∞u c·ªôt numeric
            for col in df.select_dtypes(include=['float']).columns:
                if col not in exclude_cols:
                    try:
                        col_min = df[col].min()
                        col_max = df[col].max()
                        
                        # Ki·ªÉm tra xem d·ªØ li·ªáu c√≥ th·ªÉ chuy·ªÉn ƒë·ªïi sang int kh√¥ng
                        if df[col].equals(df[col].astype(int)):
                            if col_min >= np.iinfo(np.int32).min and col_max <= np.iinfo(np.int32).max:
                                df[col] = df[col].astype(np.int32)
                            elif col_min >= np.iinfo(np.int16).min and col_max <= np.iinfo(np.int16).max:
                                df[col] = df[col].astype(np.int16)
                            elif col_min >= np.iinfo(np.int8).min and col_max <= np.iinfo(np.int8).max:
                                df[col] = df[col].astype(np.int8)
                        else:
                            # T·ªëi ∆∞u c·ªôt float
                            if col_min >= np.finfo(np.float32).min and col_max <= np.finfo(np.float32).max:
                                df[col] = df[col].astype(np.float32)
                    except Exception as e:
                        logger.warning(f"Kh√¥ng th·ªÉ t·ªëi ∆∞u h√≥a c·ªôt {col}: {e}")
                        continue
            
            # T·ªëi ∆∞u c·ªôt integer
            for col in df.select_dtypes(include=['int']).columns:
                if col not in exclude_cols:
                    try:
                        col_min = df[col].min()
                        col_max = df[col].max()
                        
                        if col_min >= np.iinfo(np.int32).min and col_max <= np.iinfo(np.int32).max:
                            df[col] = df[col].astype(np.int32)
                        elif col_min >= np.iinfo(np.int16).min and col_max <= np.iinfo(np.int16).max:
                            df[col] = df[col].astype(np.int16)
                        elif col_min >= np.iinfo(np.int8).min and col_max <= np.iinfo(np.int8).max:
                            df[col] = df[col].astype(np.int8)
                    except Exception as e:
                        logger.warning(f"Kh√¥ng th·ªÉ t·ªëi ∆∞u h√≥a c·ªôt {col}: {e}")
                        continue
            
            # T·ªëi ∆∞u c·ªôt boolean
            for col in df.select_dtypes(include=['bool']).columns:
                if col not in exclude_cols:
                    try:
                        df[col] = df[col].astype('int8')  # int8 ti·∫øt ki·ªám h∆°n bool
                    except Exception as e:
                        logger.warning(f"Kh√¥ng th·ªÉ t·ªëi ∆∞u h√≥a c·ªôt {col}: {e}")
                        continue
                        
        except Exception as e:
            logger.error(f"L·ªói khi t·ªëi ∆∞u h√≥a DataFrame: {e}")
            # Tr·∫£ v·ªÅ DataFrame g·ªëc n·∫øu x·∫£y ra l·ªói
            return df
            
        return df
    
    def _load_cached_data(self, start_date, end_date, timeframe=None):
        """
        T·∫£i d·ªØ li·ªáu ƒë√£ l∆∞u t·ª´ b·ªô nh·ªõ ƒë·ªám v·ªõi h·ªó tr·ª£ cho c·∫£ ƒë·ªãnh d·∫°ng n√©n v√† kh√¥ng n√©n.
        
        Args:
            start_date (str): Ng√†y b·∫Øt ƒë·∫ßu kho·∫£ng th·ªùi gian
            end_date (str): Ng√†y k·∫øt th√∫c kho·∫£ng th·ªùi gian
            timeframe (str, optional): Khung th·ªùi gian c·ªßa d·ªØ li·ªáu
            
        Returns:
            pd.DataFrame: D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
        """
        try:
            # T·∫°o c√°c ƒë∆∞·ªùng d·∫´n t·ªáp cache c√≥ th·ªÉ (n√©n v√† kh√¥ng n√©n)
            cache_dir = os.path.join(config.MODEL_DIR, "data_cache")
            
            # T·∫°o t√™n t·ªáp d·ª±a tr√™n kho·∫£ng th·ªùi gian v√† khung th·ªùi gian
            file_name = f"{start_date}_to_{end_date}"
            if timeframe:
                file_name += f"_{timeframe}"
            
            cache_file_gz = os.path.join(cache_dir, f"{file_name}.pkl.gz")
            cache_file = os.path.join(cache_dir, f"{file_name}.pkl")
            
            # Ki·ªÉm tra t·ªáp n√©n tr∆∞·ªõc
            if os.path.exists(cache_file_gz):
                # T·∫£i d·ªØ li·ªáu t·ª´ t·ªáp n√©n
                data = pd.read_pickle(cache_file_gz, compression='gzip')
                file_size = os.path.getsize(cache_file_gz) / (1024 * 1024)  # Convert to MB
                logger.info(f"Loaded compressed cached data for period {start_date} to {end_date} ({file_size:.2f} MB)")
                return data
            # Ki·ªÉm tra t·ªáp kh√¥ng n√©n
            elif os.path.exists(cache_file):
                # T·∫£i d·ªØ li·ªáu t·ª´ t·ªáp kh√¥ng n√©n
                data = pd.read_pickle(cache_file)
                
                # T·ªëi ∆∞u v√† l∆∞u t·ªáp n√©n cho l·∫ßn sau
                optimized_data = self._optimize_dataframe_types(data.copy())
                optimized_data.to_pickle(cache_file_gz, compression='gzip')
                
                logger.info(f"Loaded and migrated cached data for period {start_date} to {end_date}")
                return data
            
            # N·∫øu kh√¥ng t√¨m th·∫•y t·ªáp cache n√†o
            logger.info(f"No cached data found for period {start_date} to {end_date}")
        except Exception as e:
            logger.error(f"Error loading cached data: {e}")
        
        return None
    
    # Ph·∫ßn n√†y ƒë√£ ƒë∆∞·ª£c di chuy·ªÉn v√†o ph∆∞∆°ng th·ª©c _train_by_monthly_chunks
        # Sau khi x·ª≠ l√Ω to√†n b·ªô d·ªØ li·ªáu cho t·∫•t c·∫£ c√°c khung th·ªùi gian
        model_results = {}
        for timeframe, data_chunks in all_processed_data.items():
            if data_chunks:
                # K·∫øt h·ª£p t·∫•t c·∫£ c√°c ƒëo·∫°n d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω cho khung th·ªùi gian n√†y
                combined_data = pd.concat(data_chunks)
                
                # Lo·∫°i b·ªè c√°c d√≤ng tr√πng l·∫∑p
                combined_data = combined_data[~combined_data.index.duplicated(keep='last')]
                
                # S·∫Øp x·∫øp theo th·ªùi gian
                combined_data.sort_index(inplace=True)
                
                self._add_log(f"üìä T·ªïng h·ª£p d·ªØ li·ªáu ({timeframe}): {len(combined_data)} ƒëi·ªÉm d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω")
                logger.info(f"Combined data for {timeframe}: {len(combined_data)} data points")
                
                # Chu·∫©n b·ªã d·ªØ li·ªáu cho c√°c lo·∫°i m√¥ h√¨nh kh√°c nhau
                sequence_data = self.data_processor.prepare_sequence_data(combined_data)
                image_data = self.data_processor.prepare_cnn_data(combined_data)
                
                # Hu·∫•n luy·ªán t·∫•t c·∫£ c√°c m√¥ h√¨nh v·ªõi khung th·ªùi gian c·ª• th·ªÉ
                self._add_log(f"üß† B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán c√°c m√¥ h√¨nh cho {timeframe} v·ªõi {len(combined_data)} ƒëi·ªÉm d·ªØ li·ªáu")
                
                # L∆∞u th√¥ng tin v·ªÅ khung th·ªùi gian v√†o d·ªØ li·ªáu hu·∫•n luy·ªán
                for data_dict in [sequence_data, image_data]:
                    for key in data_dict:
                        if isinstance(data_dict[key], dict):
                            data_dict[key]['timeframe'] = timeframe
                
                # S·ª¨A L·ªñI: X·ª≠ l√Ω l·ªói "too many values to unpack (expected 2)"
                try:
                    # Fix #1: L∆∞u k·∫øt qu·∫£ v√†o bi·∫øn t·∫°m tr∆∞·ªõc ƒë·ªÉ ki·ªÉm tra lo·∫°i d·ªØ li·ªáu
                    result = self.model_trainer.train_all_models(sequence_data, image_data, timeframe=timeframe)
                    
                    # Fix #2: Ki·ªÉm tra xem k·∫øt qu·∫£ c√≥ ph·∫£i l√† tuple kh√¥ng, n·∫øu c√≥ th√¨ l·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n
                    if isinstance(result, tuple) and len(result) > 0:
                        models = result[0]  # L·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n (models)
                        self._add_log(f"‚ö†Ô∏è ƒê√£ t·ª± ƒë·ªông x·ª≠ l√Ω k·∫øt qu·∫£ tuple t·ª´ train_all_models")
                    else:
                        # N·∫øu kh√¥ng ph·∫£i tuple, s·ª≠ d·ª•ng k·∫øt qu·∫£ tr·ª±c ti·∫øp
                        models = result
                        
                    # Fix #3: ƒê·∫£m b·∫£o models kh√¥ng None tr∆∞·ªõc khi l∆∞u v√†o k·∫øt qu·∫£
                    if models is not None:
                        model_results[timeframe] = models
                        self._add_log(f"‚úÖ ƒê√£ hu·∫•n luy·ªán th√†nh c√¥ng m√¥ h√¨nh cho {timeframe}")
                    else:
                        self._add_log(f"‚ö†Ô∏è Hu·∫•n luy·ªán cho {timeframe} tr·∫£ v·ªÅ None")
                        model_results[timeframe] = {}
                        
                except ValueError as e:
                    # Fix #4: X·ª≠ l√Ω l·ªói 'too many values to unpack' n·∫øu v·∫´n x·∫£y ra
                    if "too many values to unpack" in str(e):
                        self._add_log(f"‚ö†Ô∏è L·ªói ƒë·ªãnh d·∫°ng k·∫øt qu·∫£: {str(e)}")
                        logger.warning(f"Value unpacking error in train_all_models: {str(e)}")
                        
                        try:
                            # L·∫•y k·∫øt qu·∫£ tr·ª±c ti·∫øp, kh√¥ng chuy·ªÉn sang list
                            result = self.model_trainer.train_all_models(sequence_data, image_data, timeframe=timeframe)
                            
                            # Ki·ªÉm tra tr∆∞·ªùng h·ª£p k·∫øt qu·∫£ l√† m·ªôt tuple
                            if isinstance(result, tuple) and len(result) > 0:
                                models = result[0]  # L·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n n·∫øu l√† tuple
                            else:
                                models = result  # S·ª≠ d·ª•ng k·∫øt qu·∫£ tr·ª±c ti·∫øp n·∫øu kh√¥ng ph·∫£i tuple
                                
                            if models is not None:
                                model_results[timeframe] = models
                                self._add_log(f"‚úÖ ƒê√£ kh·∫Øc ph·ª•c l·ªói v√† hu·∫•n luy·ªán th√†nh c√¥ng m√¥ h√¨nh cho {timeframe}")
                            else:
                                self._add_log(f"‚ö†Ô∏è K·∫øt qu·∫£ hu·∫•n luy·ªán r·ªóng cho {timeframe}")
                                model_results[timeframe] = {}
                        except Exception as inner_e:
                            self._add_log(f"‚ùå L·ªói khi x·ª≠ l√Ω k·∫øt qu·∫£ hu·∫•n luy·ªán: {str(inner_e)}")
                            logger.error(f"Error processing training result: {inner_e}")
                            model_results[timeframe] = {}
                    else:
                        # L·ªói ValueError kh√°c
                        self._add_log(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)}")
                        logger.error(f"Unknown error in train_all_models: {e}")
                        model_results[timeframe] = {}
                except Exception as e:
                    # Fix #5: X·ª≠ l√Ω c√°c ngo·∫°i l·ªá kh√°c
                    self._add_log(f"‚ùå L·ªói trong qu√° tr√¨nh hu·∫•n luy·ªán: {str(e)}")
                    logger.error(f"Error in training process: {e}")
                    model_results[timeframe] = {}
                
                # Th√¥ng b√°o k·∫øt qu·∫£ sau khi x·ª≠ l√Ω
                if timeframe in model_results and model_results[timeframe]:
                    try:
                        model_count = len(model_results[timeframe])
                        self._add_log(f"‚úÖ ƒê√£ hu·∫•n luy·ªán th√†nh c√¥ng {model_count} m√¥ h√¨nh cho {timeframe}")
                    except:
                        self._add_log(f"‚úÖ ƒê√£ hu·∫•n luy·ªán th√†nh c√¥ng m√¥ h√¨nh cho {timeframe}")
                else:
                    self._add_log(f"‚ö†Ô∏è Kh√¥ng c√≥ m√¥ h√¨nh n√†o ƒë∆∞·ª£c hu·∫•n luy·ªán cho {timeframe}")
                logger.info(f"Trained models for {timeframe} with {len(combined_data)} data points")
            else:
                self._add_log(f"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu kh·∫£ d·ª•ng cho {timeframe} sau khi x·ª≠ l√Ω t·∫•t c·∫£ c√°c ƒëo·∫°n")
                logger.error(f"No processed data available for {timeframe} after processing all chunks")
        
        return model_results
            
    def _train_with_all_data(self):
        """Train models using all data at once."""
        logger.info("Training with all data at once")
        
        # Dictionary ƒë·ªÉ l∆∞u tr·ªØ k·∫øt qu·∫£ m√¥ h√¨nh cho m·ªói khung th·ªùi gian
        model_results = {}
        
        try:
            # L·∫∑p qua t·ª´ng khung th·ªùi gian ƒë·ªÉ hu·∫•n luy·ªán
            for timeframe in self.timeframes_to_train:
                # L∆∞u th√¥ng tin v·ªÅ timeframe hi·ªán t·∫°i ƒë·ªÉ s·ª≠ d·ª•ng trong c√°c ph∆∞∆°ng th·ª©c kh√°c
                self.current_timeframe = timeframe
                
                # Collect all historical data
                raw_data = None
                
                self._add_log(f"üîÑ ƒêang thu th·∫≠p d·ªØ li·ªáu l·ªãch s·ª≠ cho {timeframe}...")
                
                try:
                    if hasattr(config, 'HISTORICAL_START_DATE') and config.HISTORICAL_START_DATE:
                        raw_data = self.data_collector.collect_historical_data(
                            timeframe=timeframe,
                            start_date=config.HISTORICAL_START_DATE
                        )
                    else:
                        raw_data = self.data_collector.collect_historical_data(
                            timeframe=timeframe,
                            limit=config.LOOKBACK_PERIODS
                        )
                except Exception as e:
                    self._add_log(f"‚ùå L·ªói khi thu th·∫≠p d·ªØ li·ªáu cho {timeframe}: {str(e)}")
                    logger.error(f"Error collecting data for {timeframe}: {e}")
                    model_results[timeframe] = {}
                    continue
                    
                if raw_data is None or raw_data.empty:
                    self._add_log(f"‚ö†Ô∏è Kh√¥ng thu th·∫≠p ƒë∆∞·ª£c d·ªØ li·ªáu cho {timeframe}")
                    model_results[timeframe] = {}
                    continue
                
                try:
                    # Process the data
                    self._add_log(f"üîß ƒêang x·ª≠ l√Ω {len(raw_data)} ƒëi·ªÉm d·ªØ li·ªáu l·ªãch s·ª≠ cho {timeframe}...")
                    processed_data = self.data_processor.process_data(raw_data)
                    
                    if processed_data is None or processed_data.empty:
                        self._add_log(f"‚ö†Ô∏è D·ªØ li·ªáu sau khi x·ª≠ l√Ω tr·ªëng cho {timeframe}")
                        model_results[timeframe] = {}
                        continue
                        
                    # Prepare data for different model types
                    self._add_log(f"üìä ƒêang chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫ßu v√†o cho c√°c m√¥ h√¨nh ({timeframe})...")
                    sequence_data = self.data_processor.prepare_sequence_data(processed_data)
                    image_data = self.data_processor.prepare_cnn_data(processed_data)
                    
                    # L∆∞u th√¥ng tin v·ªÅ khung th·ªùi gian v√†o d·ªØ li·ªáu hu·∫•n luy·ªán
                    for data_dict in [sequence_data, image_data]:
                        for key in data_dict:
                            if isinstance(data_dict[key], dict):
                                data_dict[key]['timeframe'] = timeframe
                    
                    # Train all models
                    self._add_log(f"üß† B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán c√°c m√¥ h√¨nh cho {timeframe} v·ªõi {len(processed_data)} ƒëi·ªÉm d·ªØ li·ªáu")
                    
                    # S·ª¨A L·ªñI: X·ª≠ l√Ω l·ªói "too many values to unpack (expected 2)"
                    try:
                        # Ghi log ƒë·ªÉ debug
                        self._add_log(f"üîç B·∫Øt ƒë·∫ßu g·ªçi train_all_models v·ªõi timeframe={timeframe}")
                        
                        # L∆∞u k·∫øt qu·∫£ v√†o bi·∫øn t·∫°m tr∆∞·ªõc ƒë·ªÉ ki·ªÉm tra lo·∫°i d·ªØ li·ªáu
                        result = self.model_trainer.train_all_models(sequence_data, image_data, timeframe=timeframe)
                        
                        self._add_log(f"üìã K·∫øt qu·∫£ tr·∫£ v·ªÅ t·ª´ train_all_models: {type(result)}")
                        
                        # Ki·ªÉm tra k·ªπ h∆°n xem k·∫øt qu·∫£ c√≥ ph·∫£i l√† tuple kh√¥ng v√† c√≥ chi·ªÅu d√†i ƒë·ªß kh√¥ng
                        if result is not None:
                            if isinstance(result, tuple) and len(result) > 0:
                                models = result[0]  # L·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n (models)
                                self._add_log(f"‚ö†Ô∏è ƒê√£ t·ª± ƒë·ªông x·ª≠ l√Ω k·∫øt qu·∫£ tuple t·ª´ train_all_models")
                            else:
                                # N·∫øu kh√¥ng ph·∫£i tuple, s·ª≠ d·ª•ng k·∫øt qu·∫£ tr·ª±c ti·∫øp
                                models = result
                                
                            # ƒê·∫£m b·∫£o models kh√¥ng None tr∆∞·ªõc khi l∆∞u v√†o k·∫øt qu·∫£
                            if models is not None:
                                model_results[timeframe] = models
                                self._add_log(f"‚úÖ ƒê√£ hu·∫•n luy·ªán th√†nh c√¥ng m√¥ h√¨nh cho {timeframe}")
                            else:
                                self._add_log(f"‚ùå K·∫øt qu·∫£ models l√† None sau khi x·ª≠ l√Ω")
                                model_results[timeframe] = {}
                        else:
                            self._add_log(f"‚ùå train_all_models tr·∫£ v·ªÅ None cho {timeframe}")
                            model_results[timeframe] = {}
                            
                    except ValueError as e:
                        # X·ª≠ l√Ω l·ªói 'too many values to unpack' n·∫øu v·∫´n x·∫£y ra
                        if "too many values to unpack" in str(e):
                            self._add_log(f"‚ö†Ô∏è L·ªói ƒë·ªãnh d·∫°ng k·∫øt qu·∫£: {str(e)}")
                            logger.warning(f"Value unpacking error in train_all_models: {str(e)}")
                            
                            try:
                                # L·∫•y k·∫øt qu·∫£ tr·ª±c ti·∫øp, kh√¥ng chuy·ªÉn sang list
                                result = self.model_trainer.train_all_models(sequence_data, image_data, timeframe=timeframe)
                                
                                # Ki·ªÉm tra tr∆∞·ªùng h·ª£p k·∫øt qu·∫£ l√† m·ªôt tuple
                                if isinstance(result, tuple) and len(result) > 0:
                                    models = result[0]  # L·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n n·∫øu l√† tuple
                                else:
                                    models = result  # S·ª≠ d·ª•ng k·∫øt qu·∫£ tr·ª±c ti·∫øp n·∫øu kh√¥ng ph·∫£i tuple
                                    
                                if models is not None:
                                    model_results[timeframe] = models
                                    self._add_log(f"‚úÖ ƒê√£ kh·∫Øc ph·ª•c l·ªói v√† hu·∫•n luy·ªán th√†nh c√¥ng m√¥ h√¨nh cho {timeframe}")
                                else:
                                    self._add_log(f"‚ö†Ô∏è K·∫øt qu·∫£ hu·∫•n luy·ªán r·ªóng cho {timeframe}")
                                    model_results[timeframe] = {}
                            except Exception as inner_e:
                                self._add_log(f"‚ùå L·ªói khi x·ª≠ l√Ω k·∫øt qu·∫£ hu·∫•n luy·ªán: {str(inner_e)}")
                                logger.error(f"Error processing training result: {inner_e}")
                                model_results[timeframe] = {}
                        else:
                            # L·ªói ValueError kh√°c
                            self._add_log(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)}")
                            logger.error(f"Unknown error in train_all_models: {e}")
                            model_results[timeframe] = {}
                    except Exception as e:
                        # X·ª≠ l√Ω c√°c ngo·∫°i l·ªá kh√°c
                        self._add_log(f"‚ùå L·ªói trong qu√° tr√¨nh hu·∫•n luy·ªán: {str(e)}")
                        logger.error(f"Error in training process: {e}")
                        model_results[timeframe] = {}
                    
                    # Th√¥ng b√°o k·∫øt qu·∫£ sau khi x·ª≠ l√Ω
                    if timeframe in model_results and model_results[timeframe]:
                        try:
                            model_count = len(model_results[timeframe])
                            self._add_log(f"‚úÖ ƒê√£ hu·∫•n luy·ªán th√†nh c√¥ng {model_count} m√¥ h√¨nh cho {timeframe}")
                        except:
                            self._add_log(f"‚úÖ ƒê√£ hu·∫•n luy·ªán th√†nh c√¥ng m√¥ h√¨nh cho {timeframe}")
                    else:
                        self._add_log(f"‚ö†Ô∏è Kh√¥ng c√≥ m√¥ h√¨nh n√†o ƒë∆∞·ª£c hu·∫•n luy·ªán cho {timeframe}")
                except Exception as data_process_error:
                    self._add_log(f"‚ùå L·ªói khi x·ª≠ l√Ω d·ªØ li·ªáu: {str(data_process_error)}")
                    logger.error(f"Error processing data for {timeframe}: {data_process_error}")
                    model_results[timeframe] = {}
                    
                # Ki·ªÉm tra n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu
                if raw_data is None or raw_data.empty:
                    self._add_log(f"‚ùå Kh√¥ng th·ªÉ thu th·∫≠p d·ªØ li·ªáu l·ªãch s·ª≠ cho khung th·ªùi gian {timeframe}")
                    logger.error(f"No data collected for training timeframe {timeframe}")
                    model_results[timeframe] = {}
                
        except Exception as e:
            self._add_log(f"‚ùå L·ªói hu·∫•n luy·ªán: {str(e)}")
            logger.error(f"Error training with all data: {e}")
            
        return model_results
            
    def increment_new_data_count(self, count=1):
        """
        Increment the counter of new data points.
        
        Args:
            count (int): Number of new data points to add
        """
        self.new_data_count += count
    
    def _train_for_timeframe(self, timeframe):
        """
        Train models for a specific timeframe.
        
        Args:
            timeframe (str): Timeframe to train for (e.g., "1m", "5m")
        """
        try:
            # Import dependencies here to avoid circular imports
            from utils.data_collector import create_data_collector
            from utils.data_processor import DataProcessor
            from models.model_trainer import ModelTrainer
            from utils.thread_safe_logging import thread_safe_log
            
            # Create necessary objects
            data_collector = create_data_collector()
            data_processor = DataProcessor()
            model_trainer = ModelTrainer()
            
            # Collect data
            self._add_log(f"Collecting data for {timeframe}")
            thread_safe_log(f"Collecting data for {timeframe}")
            
            if hasattr(config, 'HISTORICAL_START_DATE') and config.HISTORICAL_START_DATE:
                # For historical training
                data = data_collector.collect_historical_data(
                    timeframe=timeframe,
                    start_date=config.HISTORICAL_START_DATE
                )
            else:
                # For recent data only
                data = data_collector.collect_historical_data(
                    timeframe=timeframe, 
                    limit=config.LOOKBACK_PERIODS
                )
            
            if data is None or len(data) == 0:
                self._add_log(f"No data available for {timeframe}")
                thread_safe_log(f"No data available for {timeframe}")
                return
                
            self._add_log(f"Collected {len(data)} candles for {timeframe}")
            thread_safe_log(f"Collected {len(data)} candles for {timeframe}")
            
            # Process data
            self._add_log(f"Processing data for {timeframe}")
            thread_safe_log(f"Processing data for {timeframe}")
            processed_data = data_processor.process_data(data)
            
            # Prepare sequence data
            self._add_log(f"Preparing sequence data for {timeframe}")
            thread_safe_log(f"Preparing sequence data for {timeframe}")
            sequence_data = data_processor.prepare_sequence_data(processed_data)
            
            # Prepare image data
            self._add_log(f"Preparing image data for {timeframe}")
            thread_safe_log(f"Preparing image data for {timeframe}")
            image_data = data_processor.prepare_cnn_data(processed_data)
            
            # Train models
            self._add_log(f"Training models for {timeframe}")
            thread_safe_log(f"Training models for {timeframe}")
            
            # QUAN TR·ªåNG: S·ª¨A L·ªñI ·ªû ƒê√ÇY - V·∫•n ƒë·ªÅ "too many values to unpack"
            try:
                # C√°ch 1: G√°n tr·ª±c ti·∫øp k·∫øt qu·∫£ cho models
                models = model_trainer.train_all_models(sequence_data, image_data, timeframe=timeframe)
            except ValueError as e:
                # N·∫øu g·∫∑p l·ªói "too many values", ƒë√¢y l√† ph∆∞∆°ng ph√°p s·ª≠a thay th·∫ø
                if "too many values to unpack" in str(e):
                    thread_safe_log(f"L·ªói gi√° tr·ªã khi hu·∫•n luy·ªán: {e}, ƒëang th·ª≠ ph∆∞∆°ng ph√°p kh√°c")
                    # L∆∞u k·∫øt qu·∫£ v√†o bi·∫øn t·∫°m th·ªùi
                    result = model_trainer.train_all_models(sequence_data, image_data, timeframe=timeframe)
                    # L·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n n·∫øu l√† m·ªôt tuple
                    if isinstance(result, tuple) and len(result) > 0:
                        models = result[0]
                    else:
                        models = result
                else:
                    # N·∫øu l√† l·ªói kh√°c, n√©m l·∫°i ngo·∫°i l·ªá
                    raise
            
            if models:
                self._add_log(f"Models trained successfully for {timeframe}")
                thread_safe_log(f"Models trained successfully for {timeframe}")
                return models
            else:
                self._add_log(f"No models trained for {timeframe}")
                thread_safe_log(f"No models trained for {timeframe}")
                return None
                
        except Exception as e:
            self._add_log(f"Error training for timeframe {timeframe}: {e}")
            thread_safe_log(f"Error training for timeframe {timeframe}: {e}")
            logger.error(f"Error training for timeframe {timeframe}: {e}")
            raise

# Singleton instance
_instance = None

def get_continuous_trainer():
    """
    Get the singleton continuous trainer instance.
    
    Returns:
        ContinuousTrainer: The continuous trainer instance
    """
    global _instance
    if _instance is None:
        _instance = ContinuousTrainer()
    return _instance