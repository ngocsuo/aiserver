"""
Continuous training module for scheduling and executing model retraining.
"""
import logging
import threading
import time
import queue
import calendar
from datetime import datetime, timedelta
import os
import json
import pandas as pd
import numpy as np

import config
from utils.data_collector import create_data_collector
from utils.data_processor import DataProcessor
from models.model_trainer import ModelTrainer

# Set up logging
logger = logging.getLogger("continuous_trainer")

class ContinuousTrainer:
    """
    Manager for continuous model training.
    Schedules and executes training jobs based on configured schedule.
    """
    def __init__(self):
        """Initialize the continuous training manager."""
        self.data_collector = create_data_collector()
        self.data_processor = DataProcessor()
        self.model_trainer = ModelTrainer()
        
        self.training_thread = None
        self.stop_training = threading.Event()
        self.training_queue = queue.Queue()
        
        self.last_training_time = None
        self.training_in_progress = False
        self.training_history = []
        self.new_data_count = 0
        
        # Log storage for UI display
        self.log_messages = []
        
        # Training progress tracking
        self.current_progress = 0
        self.total_chunks = 0
        self.current_chunk = 0
        
        # Danh s√°ch c√°c khung th·ªùi gian c·∫ßn hu·∫•n luy·ªán
        self.timeframes_to_train = [config.PRIMARY_TIMEFRAME, config.SECONDARY_TIMEFRAME]
        
        # Historical start date for training (can be updated at runtime)
        self.historical_start_date = config.HISTORICAL_START_DATE
        # Monthly chunks for training
        self.monthly_chunks = self._generate_monthly_chunks()
        
        # L∆∞u danh s√°ch c√°c ng√†y b·∫Øt ƒë·∫ßu chunks ƒë·ªÉ d√πng sau n√†y
        self.chunk_start_dates = self.monthly_chunks.copy()
        self._add_log("Continuous trainer initialized with schedule: " + config.TRAINING_SCHEDULE['frequency'] + 
                     f" for timeframes: {', '.join(self.timeframes_to_train)}")
        
    def _generate_monthly_chunks(self):
        """
        Generate a list of date ranges for monthly chunks from the
        configured historical start date to the present.
        
        Returns:
            list: List of (start_date, end_date) tuples for monthly chunks
        """
        # S·ª≠ d·ª•ng gi√° tr·ªã historical_start_date c·ªßa ƒë·ªëi t∆∞·ª£ng continuous_trainer
        # ho·∫∑c backup t·ª´ config n·∫øu kh√¥ng c√≥
        if hasattr(self, 'historical_start_date') and self.historical_start_date:
            start = datetime.strptime(self.historical_start_date, "%Y-%m-%d")
            logger.info(f"Using custom historical start date: {self.historical_start_date}")
            self._add_log(f"üîç S·ª≠ d·ª•ng ng√†y b·∫Øt ƒë·∫ßu t√πy ch·ªânh: {self.historical_start_date}")
        elif hasattr(config, 'DEFAULT_TRAINING_START_DATE') and config.DEFAULT_TRAINING_START_DATE:
            # S·ª≠ d·ª•ng d·ªØ li·ªáu 12 th√°ng g·∫ßn nh·∫•t cho hu·∫•n luy·ªán
            start = datetime.strptime(config.DEFAULT_TRAINING_START_DATE, "%Y-%m-%d")
            logger.info(f"Using 12-month data for training: starting from {config.DEFAULT_TRAINING_START_DATE}")
            self._add_log(f"üîç S·ª≠ d·ª•ng d·ªØ li·ªáu 12 th√°ng g·∫ßn nh·∫•t cho hu·∫•n luy·ªán (t·ª´ {config.DEFAULT_TRAINING_START_DATE})")
        elif hasattr(config, 'HISTORICAL_START_DATE') and config.HISTORICAL_START_DATE:
            # S·ª≠ d·ª•ng ng√†y b·∫Øt ƒë·∫ßu l·ªãch s·ª≠ c≈© n·∫øu kh√¥ng c√≥ c√†i ƒë·∫∑t m·ªõi
            start = datetime.strptime(config.HISTORICAL_START_DATE, "%Y-%m-%d")
            logger.info(f"Using historical data from {config.HISTORICAL_START_DATE}")
        else:
            return []
            
        end = datetime.now()
        
        chunks = []
        current = start
        
        while current < end:
            # Get the last day of the current month
            _, last_day = calendar.monthrange(current.year, current.month)
            month_end = datetime(current.year, current.month, last_day)
            
            # If this would go beyond the end, use the end date
            if month_end > end:
                month_end = end
                
            # Format for Binance API
            start_date = current.strftime("%Y-%m-%d")
            end_date = month_end.strftime("%Y-%m-%d")
            
            chunks.append((start_date, end_date))
            
            # Move to first day of next month
            current = datetime(current.year, current.month, last_day) + timedelta(days=1)
            
        logger.info(f"Generated {len(chunks)} monthly chunks from {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}")
        return chunks
        
    def start(self):
        """Start the continuous training manager."""
        if self.training_thread is not None and self.training_thread.is_alive():
            logger.warning("Training thread is already running")
            return
            
        # Reset the stop flag
        self.stop_training.clear()
        
        # Create and start the training thread
        self.training_thread = threading.Thread(
            target=self._training_loop, 
            name="ContinuousTrainingThread",
            daemon=True
        )
        self.training_thread.start()
        
        self._add_log("üöÄ Qu√° tr√¨nh hu·∫•n luy·ªán li√™n t·ª•c ƒë√£ ƒë∆∞·ª£c kh·ªüi ƒë·ªông")
        logger.info("Continuous training manager started")
        
    def stop(self):
        """Stop the continuous training manager."""
        if self.training_thread is None or not self.training_thread.is_alive():
            logger.warning("Training thread is not running")
            return
            
        # Set the stop flag
        self.stop_training.set()
        
        # Wait for the thread to finish
        self._add_log("‚è±Ô∏è ƒêang d·ª´ng qu√° tr√¨nh hu·∫•n luy·ªán li√™n t·ª•c...")
        self.training_thread.join(timeout=5.0)
        
        if self.training_thread.is_alive():
            self._add_log("‚ö†Ô∏è Kh√¥ng th·ªÉ d·ª´ng ti·∫øn tr√¨nh hu·∫•n luy·ªán s·∫°ch s·∫Ω")
            logger.warning("Training thread did not stop cleanly")
        else:
            self._add_log("‚úÖ ƒê√£ d·ª´ng qu√° tr√¨nh hu·∫•n luy·ªán li√™n t·ª•c")
            logger.info("Continuous training manager stopped")
            
        self.training_thread = None
        
    def schedule_training(self, force=False):
        """
        Schedule a training job.
        
        Args:
            force (bool): If True, force training regardless of schedule
        """
        if force:
            self._add_log("üîÑ ƒê√£ l√™n l·ªãch hu·∫•n luy·ªán th·ªß c√¥ng")
            logger.info("Forced training scheduled")
            self.training_queue.put("FORCE_TRAIN")
        else:
            self._add_log("üîÑ ƒê√£ l√™n l·ªãch hu·∫•n luy·ªán theo th·ªùi gian ƒë√£ c·∫•u h√¨nh")
            logger.info("Training scheduled according to configured frequency")
            self.training_queue.put("TRAIN")
            
    def _add_log(self, message):
        """Add a log message to the internal log storage for UI display"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"{timestamp} - {message}"
        self.log_messages.append(log_entry)
        # Keep only the most recent 500 messages
        if len(self.log_messages) > 500:
            self.log_messages = self.log_messages[-500:]
            
    def get_training_status(self):
        """
        Get the current training status.
        
        Returns:
            dict: Training status information
        """
        # Calculate progress percentage if training is in progress
        progress = 0
        if self.training_in_progress and self.total_chunks > 0:
            progress = min(100, int((self.current_chunk / self.total_chunks) * 100))
            
        # Th√™m th√¥ng tin v·ªÅ current_chunk v√† total_chunks cho update_status
        return {
            "enabled": config.CONTINUOUS_TRAINING,
            "in_progress": self.training_in_progress,
            "last_training_time": self.last_training_time.strftime("%Y-%m-%d %H:%M:%S") if self.last_training_time else None,
            "training_history": self.training_history[-10:],  # Last 10 training events
            "new_data_points": self.new_data_count,
            "schedule": config.TRAINING_SCHEDULE,
            "is_training": self.training_in_progress,
            "progress": progress,
            "models_trained": self.last_training_time is not None,
            "current_chunk": self.current_chunk,
            "total_chunks": self.total_chunks,
            "status": "Training in progress" if self.training_in_progress else "Idle"
        }
        
    def _is_training_scheduled(self):
        """
        Check if training is due according to the configured schedule.
        
        Returns:
            bool: Whether training should occur now
        """
        if not config.CONTINUOUS_TRAINING:
            return False
            
        now = datetime.now()
        schedule = config.TRAINING_SCHEDULE
        
        if schedule["frequency"] == "hourly":
            # Check if the current minute matches the scheduled minute
            return now.minute == schedule["minute"]
            
        elif schedule["frequency"] == "daily":
            # Check if the current hour and minute match the scheduled time
            return (now.hour == schedule["hour"] and 
                    now.minute == schedule["minute"])
                    
        elif schedule["frequency"] == "weekly":
            # Check if the current day, hour, and minute match the scheduled time
            # Note: schedule uses Monday=0, Sunday=6, but datetime uses Monday=0, Sunday=6
            return (now.weekday() == schedule["day_of_week"] and
                    now.hour == schedule["hour"] and 
                    now.minute == schedule["minute"])
        
        return False
        
    def _training_loop(self):
        """Main training loop that runs in a background thread."""
        logger.info("Training loop started")
        
        check_interval = 60  # Check schedule every minute
        
        # Immediately schedule the first training to process historical data
        self.schedule_training(force=True)
        
        # Keep training continuously
        continuous_training_interval = 30 * 60  # 30 minutes between continuous training cycles
        last_continuous_training = time.time()
        
        while not self.stop_training.is_set():
            try:
                # For continuous training, don't wait for scheduled time
                current_time = time.time()
                if current_time - last_continuous_training >= continuous_training_interval:
                    # Start a new training cycle after the interval
                    mins = continuous_training_interval//60
                    self._add_log(f"üïí B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán ƒë·ªãnh k·ª≥ sau {mins} ph√∫t")
                    logger.info(f"Starting continuous training cycle after {mins} minutes")
                    self.schedule_training(force=True)
                    last_continuous_training = current_time
                
                # Check if there's a training job in the queue
                try:
                    # Non-blocking queue check with timeout
                    job = self.training_queue.get(block=True, timeout=1.0)
                    
                    # Process the training job
                    self._execute_training(force=(job == "FORCE_TRAIN"))
                    
                    # Mark job as done
                    self.training_queue.task_done()
                    
                except queue.Empty:
                    # No training job in the queue, continue
                    pass
                    
                # Sleep for a while before checking again
                for _ in range(check_interval):
                    if self.stop_training.is_set():
                        break
                    time.sleep(1)
                    
            except Exception as e:
                logger.error(f"Error in training loop: {e}")
                # Sleep a bit before retrying to avoid tight error loops
                time.sleep(10)
                
        logger.info("Training loop stopped")
        
    def _execute_training(self, force=False):
        """
        Execute the actual training process.
        
        Args:
            force (bool): If True, force training regardless of whether enough new data is available
        """
        if self.training_in_progress:
            logger.warning("Training already in progress, skipping")
            return
            
        try:
            self.training_in_progress = True
            start_time = datetime.now()
            
            logger.info(f"Starting training process at {start_time}")
            
            # Check if we have enough new data to justify retraining
            if not force and self.new_data_count < config.MINIMUM_NEW_DATA_POINTS:
                logger.info(f"Not enough new data ({self.new_data_count} points) for retraining, minimum required: {config.MINIMUM_NEW_DATA_POINTS}")
                self.training_in_progress = False
                return
                
            # Train by monthly chunks if enabled
            if config.CHUNK_BY_MONTHS and self.chunk_start_dates:
                self._train_by_monthly_chunks()
            else:
                self._train_with_all_data()
                
            # Record training event
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            self.last_training_time = end_time
            self.training_history.append({
                "start_time": start_time.strftime("%Y-%m-%d %H:%M:%S"),
                "end_time": end_time.strftime("%Y-%m-%d %H:%M:%S"),
                "duration_seconds": duration,
                "data_points": self.new_data_count,
                "forced": force
            })
            
            # Reset new data counter
            self.new_data_count = 0
            
            # Log completion with duration
            mins = int(duration // 60)
            secs = int(duration % 60)
            time_str = f"{mins} ph√∫t {secs} gi√¢y" if mins > 0 else f"{secs} gi√¢y"
            self._add_log(f"‚úÖ Qu√° tr√¨nh hu·∫•n luy·ªán ho√†n t·∫•t trong {time_str}")
            
            # Calculate next training time
            next_time = end_time + timedelta(seconds=30*60)  # 30 minutes
            self._add_log(f"‚è±Ô∏è ƒê·ª£t hu·∫•n luy·ªán ti·∫øp theo d·ª± ki·∫øn: {next_time.strftime('%H:%M:%S')}")
            
            logger.info(f"Training process completed in {duration:.1f} seconds")
            
        except Exception as e:
            logger.error(f"Error during training execution: {e}")
        finally:
            self.training_in_progress = False
            
    def _train_by_monthly_chunks(self):
        """Train models using monthly data chunks to manage memory usage for both timeframes."""
        logger.info(f"Training with {len(self.monthly_chunks)} monthly chunks from {self.historical_start_date} for timeframes: {', '.join(self.timeframes_to_train)}")
        self._add_log(f"B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán v·ªõi {len(self.monthly_chunks)} ƒëo·∫°n d·ªØ li·ªáu th√°ng t·ª´ {self.historical_start_date}")
        
        # Dictionary ƒë·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω cho m·ªói khung th·ªùi gian
        all_processed_data = {timeframe: [] for timeframe in self.timeframes_to_train}
        
        # Set total chunks for progress tracking (t·ªïng s·ªë chunks nh√¢n v·ªõi s·ªë khung th·ªùi gian)
        self.total_chunks = len(self.monthly_chunks) * len(self.timeframes_to_train)
        self.current_chunk = 0
        
        # Ki·ªÉm tra xem ƒë√£ c√≥ d·ªØ li·ªáu ƒë√£ t·∫£i tr∆∞·ªõc ƒë√≥ ch∆∞a
        existing_data_ranges = self._get_existing_data_ranges()
        
        # X·ª≠ l√Ω t·ª´ng khung th·ªùi gian
        for timeframe in self.timeframes_to_train:
            self._add_log(f"üïí ƒêang x·ª≠ l√Ω d·ªØ li·ªáu cho khung th·ªùi gian: {timeframe}")
            logger.info(f"Processing data for timeframe: {timeframe}")
            
            # Process each monthly chunk for this timeframe
            for i, chunk in enumerate(self.monthly_chunks):
                # ƒê·∫£m b·∫£o chunk l√† tuple v·ªõi 2 ph·∫ßn t·ª≠
                if isinstance(chunk, tuple) and len(chunk) == 2:
                    start_date, end_date = chunk
                else:
                    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p chunk kh√¥ng ph·∫£i l√† tuple 2 ph·∫ßn t·ª≠
                    self._add_log(f"‚ö†Ô∏è ƒê·ªãnh d·∫°ng chunk kh√¥ng h·ª£p l·ªá: {chunk}")
                    logger.warning(f"Invalid chunk format: {chunk}")
                    continue

                self.current_chunk += 1
                chunk_progress = int((self.current_chunk / self.total_chunks) * 100)
                
                # Kh√≥a t√†i nguy√™n cho khung th·ªùi gian v√† kho·∫£ng th·ªùi gian c·ª• th·ªÉ
                cache_key = f"{timeframe}_{start_date}_{end_date}"
                
                # Ki·ªÉm tra xem d·ªØ li·ªáu cho kho·∫£ng th·ªùi gian n√†y ƒë√£ ƒë∆∞·ª£c t·∫£i tr∆∞·ªõc ƒë√≥ ch∆∞a
                if self._is_data_range_covered(start_date, end_date, existing_data_ranges):
                    # D·ªØ li·ªáu ƒë√£ t·ªìn t·∫°i, s·ª≠ d·ª•ng l·∫°i
                    log_msg = f"‚è© B·ªè qua ƒëo·∫°n {i+1}/{len(self.monthly_chunks)} ({timeframe}): t·ª´ {start_date} ƒë·∫øn {end_date} - ƒë√£ c√≥ d·ªØ li·ªáu"
                    self._add_log(log_msg)
                    logger.info(f"Skipping chunk {i+1}/{len(self.monthly_chunks)} ({timeframe}): {start_date} to {end_date} - data already exists")
                    
                    # T·∫£i d·ªØ li·ªáu ƒë√£ l∆∞u t·ª´ t·ªáp cache
                    try:
                        cached_data = self._load_cached_data(start_date, end_date, timeframe)
                        if cached_data is not None and not cached_data.empty:
                            if timeframe not in all_processed_data:
                                all_processed_data[timeframe] = []
                            all_processed_data[timeframe].append(cached_data)
                            self._add_log(f"‚úÖ ƒêo·∫°n {i+1} ({timeframe}): ƒê√£ t·∫£i {len(cached_data)} ƒëi·ªÉm d·ªØ li·ªáu t·ª´ b·ªô nh·ªõ ƒë·ªám")
                    except Exception as e:
                        # N·∫øu kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu t·ª´ cache, t·∫£i l·∫°i t·ª´ API
                        log_msg = f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu ƒë·ªám cho ƒëo·∫°n {i+1} ({timeframe}): {str(e)} - ƒêang t·∫£i l·∫°i t·ª´ Binance"
                        self._add_log(log_msg)
                        logger.warning(f"Could not load cached data for chunk {i+1} ({timeframe}): {e} - Redownloading")
                        # Ti·∫øp t·ª•c v·ªõi quy tr√¨nh t·∫£i m·ªõi d∆∞·ªõi ƒë√¢y
                
                # N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªám ho·∫∑c kh√¥ng th·ªÉ t·∫£i, t·∫£i m·ªõi t·ª´ API
                if len(all_processed_data[timeframe]) <= i:
                    log_msg = f"üì• ƒêang t·∫£i ƒëo·∫°n d·ªØ li·ªáu {i+1}/{len(self.monthly_chunks)} ({timeframe}): t·ª´ {start_date} ƒë·∫øn {end_date} - {chunk_progress}% ho√†n th√†nh"
                    self._add_log(log_msg)
                    logger.info(f"Downloading chunk {i+1}/{len(self.monthly_chunks)} ({timeframe}): {start_date} to {end_date}")
                
                    try:
                        # Collect data for this month with the specific timeframe
                        raw_data = self.data_collector.collect_historical_data(
                            timeframe=timeframe,
                            start_date=start_date,
                            end_date=end_date
                        )
                        
                        if raw_data is not None and not raw_data.empty:
                            # Process the data
                            processed_chunk = self.data_processor.process_data(raw_data)
                            all_processed_data[timeframe].append(processed_chunk)
                            
                            # L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o b·ªô nh·ªõ ƒë·ªám v·ªõi khung th·ªùi gian
                            self._save_cached_data(processed_chunk, start_date, end_date, timeframe=timeframe)
                            
                            # C·∫≠p nh·∫≠t danh s√°ch c√°c kho·∫£ng th·ªùi gian ƒë√£ t·∫£i
                            existing_data_ranges.append((start_date, end_date, timeframe))
                            
                            self._add_log(f"‚úÖ ƒêo·∫°n {i+1} ({timeframe}): ƒê√£ x·ª≠ l√Ω {len(processed_chunk)} ƒëi·ªÉm d·ªØ li·ªáu th√†nh c√¥ng")
                            logger.info(f"Chunk {i+1} ({timeframe}): Processed {len(processed_chunk)} data points")
                        else:
                            error_msg = f"‚ö†Ô∏è ƒêo·∫°n {i+1} ({timeframe}): Kh√¥ng c√≥ d·ªØ li·ªáu cho giai ƒëo·∫°n {start_date} ƒë·∫øn {end_date}"
                            self._add_log(error_msg)
                            logger.warning(f"Chunk {i+1} ({timeframe}): No data collected for period {start_date} to {end_date}")
                            
                    except Exception as e:
                        error_msg = f"‚ùå L·ªói x·ª≠ l√Ω ƒëo·∫°n {i+1} ({timeframe}): {str(e)}"
                        self._add_log(error_msg)
                        logger.error(f"Error processing chunk {i+1} ({timeframe}): {e}")
    
    def _get_existing_data_ranges(self):
        """
        Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ t·∫£i t·ª´ tr∆∞·ªõc ƒë√≥
        
        Returns:
            list: Danh s√°ch c√°c kho·∫£ng th·ªùi gian ƒë√£ t·∫£i, d·∫°ng [(start_date, end_date), ...]
        """
        # Ki·ªÉm tra th∆∞ m·ª•c l∆∞u tr·ªØ d·ªØ li·ªáu
        cache_dir = os.path.join(config.MODEL_DIR, "data_cache")
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir, exist_ok=True)
            return []
        
        # ƒê·ªçc danh s√°ch kho·∫£ng th·ªùi gian ƒë√£ t·∫£i
        ranges_file = os.path.join(cache_dir, "data_ranges.json")
        if os.path.exists(ranges_file):
            try:
                with open(ranges_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Error loading existing data ranges: {e}")
                return []
        return []
    
    def _is_data_range_covered(self, start_date, end_date, existing_ranges):
        """
        Ki·ªÉm tra xem kho·∫£ng th·ªùi gian ƒë√£ ƒë∆∞·ª£c t·∫£i tr∆∞·ªõc ƒë√≥ ch∆∞a
        
        Args:
            start_date (str): Ng√†y b·∫Øt ƒë·∫ßu kho·∫£ng th·ªùi gian m·ªõi
            end_date (str): Ng√†y k·∫øt th√∫c kho·∫£ng th·ªùi gian m·ªõi
            existing_ranges (list): Danh s√°ch c√°c kho·∫£ng th·ªùi gian ƒë√£ t·∫£i
            
        Returns:
            bool: True n·∫øu kho·∫£ng th·ªùi gian ƒë√£ ƒë∆∞·ª£c t·∫£i, False n·∫øu ch∆∞a
        """
        start = datetime.strptime(start_date, "%Y-%m-%d")
        end = datetime.strptime(end_date, "%Y-%m-%d")
        
        for exist_start, exist_end in existing_ranges:
            exist_start_date = datetime.strptime(exist_start, "%Y-%m-%d")
            exist_end_date = datetime.strptime(exist_end, "%Y-%m-%d")
            
            # Kho·∫£ng th·ªùi gian ƒë√£ ƒë∆∞·ª£c bao ph·ªß b·ªüi m·ªôt kho·∫£ng th·ªùi gian hi·ªán c√≥
            if start >= exist_start_date and end <= exist_end_date:
                return True
        
        return False
    
    def _save_cached_data(self, data, start_date, end_date, timeframe=None):
        """
        L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o b·ªô nh·ªõ ƒë·ªám v·ªõi t√≠nh nƒÉng n√©n ƒë·ªÉ ti·∫øt ki·ªám kh√¥ng gian.
        
        Args:
            data (pd.DataFrame): D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
            start_date (str): Ng√†y b·∫Øt ƒë·∫ßu kho·∫£ng th·ªùi gian
            end_date (str): Ng√†y k·∫øt th√∫c kho·∫£ng th·ªùi gian
            timeframe (str, optional): Khung th·ªùi gian c·ªßa d·ªØ li·ªáu
        """
        try:
            # ƒê·∫£m b·∫£o th∆∞ m·ª•c cache t·ªìn t·∫°i
            cache_dir = os.path.join(config.MODEL_DIR, "data_cache")
            os.makedirs(cache_dir, exist_ok=True)
            
            # T·∫°o t√™n t·ªáp d·ª±a tr√™n kho·∫£ng th·ªùi gian v√† khung th·ªùi gian
            file_name = f"{start_date}_to_{end_date}"
            if timeframe:
                file_name += f"_{timeframe}"
            cache_file = os.path.join(cache_dir, f"{file_name}.pkl.gz")
            
            # T·ªëi ∆∞u h√≥a ki·ªÉu d·ªØ li·ªáu tr∆∞·ªõc khi l∆∞u ƒë·ªÉ gi·∫£m k√≠ch th∆∞·ªõc
            optimized_data = self._optimize_dataframe_types(data.copy())
            
            # L∆∞u DataFrame v√†o t·ªáp v·ªõi t√≠nh nƒÉng n√©n
            optimized_data.to_pickle(cache_file, compression='gzip')
            
            # T√≠nh k√≠ch th∆∞·ªõc ƒë√£ ti·∫øt ki·ªám
            normal_size = data.memory_usage(deep=True).sum()
            optimized_size = optimized_data.memory_usage(deep=True).sum()
            savings_percent = ((normal_size - optimized_size) / normal_size * 100) if normal_size > 0 else 0
            
            # C·∫≠p nh·∫≠t danh s√°ch kho·∫£ng th·ªùi gian ƒë√£ t·∫£i
            ranges_file = os.path.join(cache_dir, "data_ranges.json")
            existing_ranges = self._get_existing_data_ranges()
            
            # Th√™m kho·∫£ng th·ªùi gian m·ªõi k√®m th√¥ng tin v·ªÅ k√≠ch th∆∞·ªõc v√† ng√†y t·∫°o
            if not self._is_data_range_covered(start_date, end_date, existing_ranges):
                existing_ranges.append({
                    "start_date": start_date,
                    "end_date": end_date,
                    "created_at": datetime.now().isoformat(),
                    "size_bytes": os.path.getsize(cache_file) if os.path.exists(cache_file) else 0,
                    "rows": len(optimized_data)
                })
                
                # L∆∞u danh s√°ch c·∫≠p nh·∫≠t
                with open(ranges_file, 'w') as f:
                    json.dump(existing_ranges, f, indent=2)
            
            logger.info(f"Cached data saved for period {start_date} to {end_date} (Memory optimized: {savings_percent:.1f}% saved)")
        except Exception as e:
            logger.error(f"Error saving cached data: {e}")
    
    def _optimize_dataframe_types(self, df):
        """
        T·ªëi ∆∞u ki·ªÉu d·ªØ li·ªáu c·ªßa DataFrame ƒë·ªÉ gi·∫£m b·ªô nh·ªõ s·ª≠ d·ª•ng.
        
        Args:
            df (pd.DataFrame): DataFrame c·∫ßn t·ªëi ∆∞u
            
        Returns:
            pd.DataFrame: DataFrame ƒë√£ t·ªëi ∆∞u
        """
        # Danh s√°ch c√°c c·ªôt ƒë·ªÉ b·ªè qua qu√° tr√¨nh t·ªëi ∆∞u (c√°c c·ªôt m·ª•c ti√™u)
        exclude_cols = ['target', 'target_class', 'target_binary']
        
        # T·ªëi ∆∞u c·ªôt numeric
        for col in df.select_dtypes(include=['float']).columns:
            if col not in exclude_cols:
                col_min = df[col].min()
                col_max = df[col].max()
                
                # Ki·ªÉm tra xem d·ªØ li·ªáu c√≥ th·ªÉ chuy·ªÉn ƒë·ªïi sang int kh√¥ng
                if df[col].equals(df[col].astype(int)):
                    if col_min >= np.iinfo(np.int8).min and col_max <= np.iinfo(np.int8).max:
                        df[col] = df[col].astype(np.int8)
                    elif col_min >= np.iinfo(np.int16).min and col_max <= np.iinfo(np.int16).max:
                        df[col] = df[col].astype(np.int16)
                    elif col_min >= np.iinfo(np.int32).min and col_max <= np.iinfo(np.int32).max:
                        df[col] = df[col].astype(np.int32)
                else:
                    # T·ªëi ∆∞u c·ªôt float
                    if col_min >= np.finfo(np.float32).min and col_max <= np.finfo(np.float32).max:
                        df[col] = df[col].astype(np.float32)
        
        # T·ªëi ∆∞u c·ªôt integer
        for col in df.select_dtypes(include=['int']).columns:
            if col not in exclude_cols:
                col_min = df[col].min()
                col_max = df[col].max()
                
                if col_min >= np.iinfo(np.int8).min and col_max <= np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif col_min >= np.iinfo(np.int16).min and col_max <= np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif col_min >= np.iinfo(np.int32).min and col_max <= np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
        
        # T·ªëi ∆∞u c·ªôt boolean
        for col in df.select_dtypes(include=['bool']).columns:
            if col not in exclude_cols:
                df[col] = df[col].astype('int8')  # int8 ti·∫øt ki·ªám h∆°n bool
        
        return df
    
    def _load_cached_data(self, start_date, end_date, timeframe=None):
        """
        T·∫£i d·ªØ li·ªáu ƒë√£ l∆∞u t·ª´ b·ªô nh·ªõ ƒë·ªám v·ªõi h·ªó tr·ª£ cho c·∫£ ƒë·ªãnh d·∫°ng n√©n v√† kh√¥ng n√©n.
        
        Args:
            start_date (str): Ng√†y b·∫Øt ƒë·∫ßu kho·∫£ng th·ªùi gian
            end_date (str): Ng√†y k·∫øt th√∫c kho·∫£ng th·ªùi gian
            timeframe (str, optional): Khung th·ªùi gian c·ªßa d·ªØ li·ªáu
            
        Returns:
            pd.DataFrame: D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
        """
        try:
            # T·∫°o c√°c ƒë∆∞·ªùng d·∫´n t·ªáp cache c√≥ th·ªÉ (n√©n v√† kh√¥ng n√©n)
            cache_dir = os.path.join(config.MODEL_DIR, "data_cache")
            
            # T·∫°o t√™n t·ªáp d·ª±a tr√™n kho·∫£ng th·ªùi gian v√† khung th·ªùi gian
            file_name = f"{start_date}_to_{end_date}"
            if timeframe:
                file_name += f"_{timeframe}"
            
            cache_file_gz = os.path.join(cache_dir, f"{file_name}.pkl.gz")
            cache_file = os.path.join(cache_dir, f"{file_name}.pkl")
            
            # Ki·ªÉm tra t·ªáp n√©n tr∆∞·ªõc
            if os.path.exists(cache_file_gz):
                # T·∫£i d·ªØ li·ªáu t·ª´ t·ªáp n√©n
                data = pd.read_pickle(cache_file_gz, compression='gzip')
                file_size = os.path.getsize(cache_file_gz) / (1024 * 1024)  # Convert to MB
                logger.info(f"Loaded compressed cached data for period {start_date} to {end_date} ({file_size:.2f} MB)")
                return data
            # Ki·ªÉm tra t·ªáp kh√¥ng n√©n
            elif os.path.exists(cache_file):
                # T·∫£i d·ªØ li·ªáu t·ª´ t·ªáp kh√¥ng n√©n
                data = pd.read_pickle(cache_file)
                
                # T·ªëi ∆∞u v√† l∆∞u t·ªáp n√©n cho l·∫ßn sau
                optimized_data = self._optimize_dataframe_types(data.copy())
                optimized_data.to_pickle(cache_file_gz, compression='gzip')
                
                logger.info(f"Loaded and migrated cached data for period {start_date} to {end_date}")
                return data
            
            # N·∫øu kh√¥ng t√¨m th·∫•y t·ªáp cache n√†o
            logger.info(f"No cached data found for period {start_date} to {end_date}")
        except Exception as e:
            logger.error(f"Error loading cached data: {e}")
        
        return None
        
        # Sau khi x·ª≠ l√Ω to√†n b·ªô d·ªØ li·ªáu cho t·∫•t c·∫£ c√°c khung th·ªùi gian
        model_results = {}
        for timeframe, data_chunks in all_processed_data.items():
            if data_chunks:
                # K·∫øt h·ª£p t·∫•t c·∫£ c√°c ƒëo·∫°n d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω cho khung th·ªùi gian n√†y
                combined_data = pd.concat(data_chunks)
                
                # Lo·∫°i b·ªè c√°c d√≤ng tr√πng l·∫∑p
                combined_data = combined_data[~combined_data.index.duplicated(keep='last')]
                
                # S·∫Øp x·∫øp theo th·ªùi gian
                combined_data.sort_index(inplace=True)
                
                self._add_log(f"üìä T·ªïng h·ª£p d·ªØ li·ªáu ({timeframe}): {len(combined_data)} ƒëi·ªÉm d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω")
                logger.info(f"Combined data for {timeframe}: {len(combined_data)} data points")
                
                # Chu·∫©n b·ªã d·ªØ li·ªáu cho c√°c lo·∫°i m√¥ h√¨nh kh√°c nhau
                sequence_data = self.data_processor.prepare_sequence_data(combined_data)
                image_data = self.data_processor.prepare_cnn_data(combined_data)
                
                # Hu·∫•n luy·ªán t·∫•t c·∫£ c√°c m√¥ h√¨nh v·ªõi khung th·ªùi gian c·ª• th·ªÉ
                self._add_log(f"üß† B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán c√°c m√¥ h√¨nh cho {timeframe} v·ªõi {len(combined_data)} ƒëi·ªÉm d·ªØ li·ªáu")
                
                # L∆∞u th√¥ng tin v·ªÅ khung th·ªùi gian v√†o d·ªØ li·ªáu hu·∫•n luy·ªán
                for data_dict in [sequence_data, image_data]:
                    for key in data_dict:
                        if isinstance(data_dict[key], dict):
                            data_dict[key]['timeframe'] = timeframe
                
                models = self.model_trainer.train_all_models(sequence_data, image_data, timeframe=timeframe)
                model_results[timeframe] = models
                
                self._add_log(f"‚úÖ ƒê√£ hu·∫•n luy·ªán th√†nh c√¥ng {len(models)} m√¥ h√¨nh cho {timeframe}")
                logger.info(f"Trained {len(models)} models for {timeframe} with {len(combined_data)} data points")
            else:
                self._add_log(f"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu kh·∫£ d·ª•ng cho {timeframe} sau khi x·ª≠ l√Ω t·∫•t c·∫£ c√°c ƒëo·∫°n")
                logger.error(f"No processed data available for {timeframe} after processing all chunks")
        
        # Tr·∫£ v·ªÅ k·∫øt qu·∫£ hu·∫•n luy·ªán cho t·∫•t c·∫£ c√°c khung th·ªùi gian
        return model_results
            
    def _train_with_all_data(self):
        """Train models using all data at once."""
        logger.info("Training with all data at once")
        
        try:
            # Collect all historical data
            raw_data = None
            
            self._add_log("üîÑ ƒêang thu th·∫≠p d·ªØ li·ªáu l·ªãch s·ª≠...")
            
            if hasattr(config, 'HISTORICAL_START_DATE') and config.HISTORICAL_START_DATE:
                raw_data = self.data_collector.collect_historical_data(
                    timeframe=config.TIMEFRAMES["primary"],
                    start_date=config.HISTORICAL_START_DATE
                )
            else:
                raw_data = self.data_collector.collect_historical_data(
                    timeframe=config.TIMEFRAMES["primary"],
                    limit=config.LOOKBACK_PERIODS
                )
                
            if raw_data is not None and not raw_data.empty:
                # Process the data
                self._add_log(f"üîß ƒêang x·ª≠ l√Ω {len(raw_data)} ƒëi·ªÉm d·ªØ li·ªáu l·ªãch s·ª≠...")
                processed_data = self.data_processor.process_data(raw_data)
                
                # Prepare data for different model types
                self._add_log("üìä ƒêang chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫ßu v√†o cho c√°c m√¥ h√¨nh...")
                sequence_data = self.data_processor.prepare_sequence_data(processed_data)
                image_data = self.data_processor.prepare_cnn_data(processed_data)
                
                # Train all models
                self._add_log(f"üß† B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán c√°c m√¥ h√¨nh v·ªõi {len(processed_data)} ƒëi·ªÉm d·ªØ li·ªáu")
                models = self.model_trainer.train_all_models(sequence_data, image_data)
                
                self._add_log(f"‚úÖ ƒê√£ hu·∫•n luy·ªán th√†nh c√¥ng {len(models)} m√¥ h√¨nh")
                logger.info(f"Trained {len(models)} models with {len(processed_data)} data points")
            else:
                self._add_log("‚ùå Kh√¥ng th·ªÉ thu th·∫≠p d·ªØ li·ªáu l·ªãch s·ª≠ cho vi·ªác hu·∫•n luy·ªán")
                logger.error("No data collected for training")
                
        except Exception as e:
            self._add_log(f"‚ùå L·ªói hu·∫•n luy·ªán: {str(e)}")
            logger.error(f"Error training with all data: {e}")
            
    def increment_new_data_count(self, count=1):
        """
        Increment the counter of new data points.
        
        Args:
            count (int): Number of new data points to add
        """
        self.new_data_count += count

# Singleton instance
_instance = None

def get_continuous_trainer():
    """
    Get the singleton continuous trainer instance.
    
    Returns:
        ContinuousTrainer: The continuous trainer instance
    """
    global _instance
    if _instance is None:
        _instance = ContinuousTrainer()
    return _instance